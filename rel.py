# encoding=utf-8
# authorï¼š s0mE
# subjectï¼š äººåä»¥åŠå…³ç³»æå–
# dateï¼š 2019-06-26
import argparse
import os
import re
import json
import time
from collections import defaultdict

from tqdm import tqdm
import numpy as np
import networkx as nx
import matplotlib
# åœ¨ CI ç¯å¢ƒä¸­ä½¿ç”¨éäº¤äº’å¼åç«¯
if os.getenv('CI') == 'true' or os.getenv('DISPLAY') is None:
    matplotlib.use('Agg')
import matplotlib.pyplot as plt
from pyhanlp import *

# å°è¯•å¯¼å…¥ OpenAI åº“ï¼ˆç”¨äºè°ƒç”¨ DeepSeek APIï¼‰
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ openai åº“æœªå®‰è£…ï¼ŒLLM åˆ†æåŠŸèƒ½ä¸å¯ç”¨ã€‚å®‰è£…æ–¹æ³•: pip install openai")

# å°è¯•å¯¼å…¥ pandasï¼ˆç”¨äº Excel å¯¼å‡ºï¼‰
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("âš ï¸ pandas åº“æœªå®‰è£…ï¼ŒExcel å¯¼å‡ºåŠŸèƒ½ä¸å¯ç”¨ã€‚å®‰è£…æ–¹æ³•: pip install pandas openpyxl")



# è®¾ç½®ä¸­æ–‡å­—ä½“ - å°è¯•å¤šä¸ªå­—ä½“é€‰é¡¹ä»¥æ”¯æŒä¸åŒç¯å¢ƒ
# ä¼˜å…ˆä½¿ç”¨ SimHeiï¼ˆWindows/æœ¬åœ°ï¼‰ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨å…¶ä»–æ”¯æŒä¸­æ–‡çš„å­—ä½“
import matplotlib.font_manager as fm

# è·å–ç³»ç»Ÿå¯ç”¨çš„ä¸­æ–‡å­—ä½“
chinese_fonts = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
                 'Noto Sans CJK SC', 'Source Han Sans CN', 'Droid Sans Fallback', 'DejaVu Sans']
available_fonts = [f.name for f in fm.fontManager.ttflist]

# æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
font_found = None
for font in chinese_fonts:
    if font in available_fonts:
        font_found = font
        break

if font_found:
    plt.rcParams["font.sans-serif"] = [font_found] + plt.rcParams["font.sans-serif"]
else:
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“å¹¶å°è¯•è®¾ç½®
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"] + plt.rcParams["font.sans-serif"]
    print("âš ï¸ Warning: No Chinese font found, Chinese characters may display as squares")

plt.rcParams["axes.unicode_minus"] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·


class hanlp(object):
    def __init__(self, analyzer = "Perceptron", custom_dict = True ):
        ## æ•°æ®é›†ç›®å½• - åŠ¨æ€è·å– pyhanlp å®‰è£…è·¯å¾„
        import pyhanlp
        
        # è·å– pyhanlp çš„å®‰è£…è·¯å¾„
        pyhanlp_dir = os.path.dirname(pyhanlp.__file__)
        static_dir = os.path.join(pyhanlp_dir, 'static')
        data_path = os.path.join(static_dir, 'data', 'model', 'perceptron', 'large', 'cws.bin')
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ HanLP çš„é»˜è®¤é…ç½®ï¼ˆè®© HanLP è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®æ–‡ä»¶ï¼‰
        if not os.path.exists(data_path):
            data_path = None
        
        ## æ„é€ äººååˆ†æå™¨
        # å¸¸è§„è¯†åˆ«
        # self.analyzer = HanLP.newSegment().enableNameRecognize(True)

        # # crfè¯†åˆ«
        self.CRFLAnalyzer = JClass("com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer")()

        #æ„ŸçŸ¥æœºè¯†åˆ«
        _PLAnalyzer = JClass("com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer")
        if data_path:
            # ä½¿ç”¨æŒ‡å®šçš„è·¯å¾„
            self.PLAnalyzer = _PLAnalyzer(
                data_path, HanLP.Config.PerceptronPOSModelPath, HanLP.Config.PerceptronNERModelPath)
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆHanLP ä¼šè‡ªåŠ¨æŸ¥æ‰¾æ•°æ®æ–‡ä»¶ï¼‰
            self.PLAnalyzer = _PLAnalyzer()
        
        self.analyzer = self.PLAnalyzer
        if analyzer=="Perceptron":
            self.analyzer = self.PLAnalyzer.enableCustomDictionary(custom_dict)
        elif analyzer=="CRF":
            self.analyzer = self.CRFLAnalyzer.enableCustomDictionary(custom_dict)
        
        # Cache JString class for type conversion
        self.JString = JClass("java.lang.String")
        
    def cut(self, words):
        res = []
        # Convert Python string to Java String for JPype1 compatibility
        # This resolves ambiguous overload between seg(String) and seg(char[])
        if isinstance(words, str):
            words = self.JString(words)
        
        if self.analyzer is None:
            terms = HanLP.segment(words)
        else:
            # Use explicit method call to avoid overload ambiguity
            terms = self.analyzer.seg(words)
        for term in terms:
            res.append( (str(term.word),str(term.nature)) )
        return res
    
    @classmethod
    def add(self,names_list):
        for n in names_list:
            if CustomDictionary.get(n) is None:
                CustomDictionary.add(n,"nr 1000 ")
            else:
                attr = "nr 1000 " + str(CustomDictionary.get(n))
                # attr = "nr 1000 "
                CustomDictionary.insert(n,attr)

    @classmethod
    def insert(self, names_list):
        for n in names_list:
            CustomDictionary.insert(n, "nr 1")
            
def count_names(fp,model):
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­çš„æ‰€æœ‰åå­—ï¼Œè¿”å›ç»Ÿè®¡çŸ©é˜µ
    """
    #é€è¡Œæå–åå­—
    name_set = set() # æ‰€æœ‰åå­—çš„é›†åˆ
    
    
    nr_nrf_dict = {"nr":{},"nrf":{}}

    cut_result = []
    lines = []

    try:
        with open(fp, "r") as f:
            lines = f.readlines()
    except UnicodeDecodeError:
        with open(fp,"r",encoding="gbk") as f:
            lines = f.readlines()

    for line in tqdm(lines, desc="Analyzing"):
        #æ¯ä¸€è¡Œåšé¢„å¤„ç†
        line = line.strip().replace(" ","")

        words = model.cut(line)
        line_dict = {}

        for word, flag in words:
            # if word == "å¼ ":
            #     print(word,flag,"|||",line)
            
            if flag == "nr" or flag == "nrf":# or flag == "j":
                # å¦‚æœ word æ˜¯äººåï¼ŒåŠ å…¥äººåçš„ç»Ÿè®¡ä¸­
                line_dict[word] = line_dict.get(word, 0) + 1
                name_set.add(word)

                # åˆ†ä¸­æ–‡åå’Œè‹±æ–‡åç»Ÿè®¡åç§°
                nr_nrf_dict[flag][word] = nr_nrf_dict[flag].get(word, 0) + 1
                
        if len(line_dict) != 0:
            cut_result.append(line_dict)

    # åå­—å…³ç³»çŸ©é˜µè®¡ç®—
    names = list(name_set)  # æ‰€æœ‰åå­—çš„åˆ—è¡¨
    name_arr = np.zeros((len(names), len(cut_result)),
                        dtype=np.int32)  # å‚¨å­˜ç»Ÿè®¡ç»“æœçš„æ•°ç»„
    for n, n_dict in enumerate(cut_result):
            for k, v in n_dict.items():
                i = names.index(k)
                name_arr[i, n] += v
    # è®¡ç®—äººåçš„å…³ç³»çŸ©é˜µ
    names = np.array(names)
    rel = np.zeros((len(names), len(names)), dtype=np.int32)
    for i in range(len(names)):
        rel[i, :] = np.sum(name_arr[:, name_arr[i, :] > 0], axis=1)

    ########è‡³æ­¤ï¼Œå·²ç»åˆæ­¥å®Œæˆäº†æ–‡ç« çš„äººç‰©å…³ç³»ç»Ÿè®¡##############
    ############ ä¸è¿‡è¿™é‡Œä»ç„¶æœ‰å¾ˆå¤šé—®é¢˜   ###################
    #### ä¾‹å¦‚æ˜æ˜¾çš„é”™è¯¯åå­—ï¼Œä»¥åŠåŒä¸€äººç‰©ä¸åŒçš„åˆ«ç§°éœ€è¦è¿›ä¸€æ­¥å¤„ç† ###
    ################éœ€è¦åç»­çš„å¤„ç† #######################
    return rel, names, nr_nrf_dict


def filter_nr(nr_nrf_dict, threshold = -1,first=False):
    """
    è‡ªåŠ¨ç”Ÿæˆå¯ä¿¡åç§°åˆ—è¡¨ å’Œ åå­—è½¬æ¢å­—å…¸
    """
    nr_dict = nr_nrf_dict["nr"]
    nrf_dict = nr_nrf_dict["nrf"]
    
    first_threshold = 5
    if threshold == -1:
        threshold = np.mean( list(nr_dict.values())+list(nrf_dict.values()))
        first_threshold = max(np.sqrt(len(nr_dict)+len(nrf_dict)),5*threshold)
    print("auto_dict threshold:{:.3f}".format(threshold))
    names = []
    trans_dict = {}
    last_names = []
    last_repeat = []

    first_names = []
    first_repeat = []
    for name,value in sorted(nr_dict.items(), key=lambda d: d[1], reverse=True):
        if value > threshold:
            if len(name) == 1 and value < first_threshold:
                continue
            names.append(name)
            last_name = name[1:]
            # è·å–ä¸‰å­—å§“åçš„åå­—çš„éƒ¨åˆ†ï¼Œå¦‚æœå­˜åœ¨é‡å¤çš„åˆ é™¤
            if len(name)==3 and not last_name in last_repeat:
                if last_name in last_names:
                    last_names.remove(last_name)
                    trans_dict.pop(last_name)
                    last_repeat.append(last_name)
                else:
                    trans_dict[last_name] = name
                    last_names.append(last_name)
            
            # è·å–å§“åçš„å§“çš„éƒ¨åˆ†
            first_name = name[:1]
            if first and len(name)==3 and not first_name in first_repeat:
                if first_name in first_names:
                    first_names.remove(first_name)
                    trans_dict.pop(first_name)
                    first_repeat.append(first_name)
                else:
                    trans_dict[first_name] = name
                    first_names.append(first_name)
        
    names = last_names + names
    # print(names)
    for name,value in nrf_dict.items():
        if value > threshold:
            names.append(name)
    return names,trans_dict

def filter_names(rel, names, trans={}, err=[], threshold= -1):
    """å¯¹ç»“æœè¿›è¡Œç²¾ç»†çš„è°ƒæ•´ä¸è¿‡æ»¤

    å¤„ç†é¡ºåº: è½¬æ¢ ==> å»é”™ ==> å»é‡ï¼ˆå­ä¸²åˆå¹¶ï¼‰==> è¿‡æ»¤ ==> æ’åº

    Args:
        rel:å…³ç³»çŸ©é˜µ n x n
        names: äººåå‘é‡çŸ©é˜µ n
        trans: åˆ«ç§°è½¬æ¢å­—å…¸ å°†åˆ«ç§°è½¬æ¢ä¸ºç»Ÿä¸€åå­—
        err: é”™è¯¯åç§°çŸ©é˜µ è¦åˆ é™¤çš„é”™è¯¯åç§°åˆ—è¡¨
        threshold: è¯é¢‘é˜ˆå€¼ è¯é¢‘ä½äºæ­¤é˜ˆå€¼çš„åå­—ä¼šè¢«è¿‡æ»¤ï¼Œç­‰äº-1ï¼ˆdefaultï¼‰æ—¶ä½¿ç”¨è¯é¢‘å‡å€¼è‡ªåŠ¨è¿‡æ»¤
    
    Returns:
        rel_filter
        names_filter
        è¿‡æ»¤å¥½çš„äººåçŸ©é˜µå’Œåç§°çŸ©é˜µ
    """
    
    rel = np.copy(rel)
    names = np.copy(names)

    # åå­—çš„è½¬æ¢ä¸è®¡æ•°çš„åˆå¹¶
    if len(trans) != 0:
        name_new = list(set(names) - set(trans.keys()))  # è½¬æ¢åçš„åå­—
        indexes = [list(names).index(n) for n in name_new]
        for i, name in enumerate(names):
            if name in trans.keys():
                new_i = list(names).index(trans[names[i]])
                rel[new_i, :] += rel[i, :]
                rel[:, new_i] += rel[:, i]
        names = np.array(name_new)
        rel = rel[indexes, :][:, indexes]

    # å»é”™
    # è‡ªåŠ¨è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯äººåçš„å­—ç¬¦ï¼ˆå¦‚çœç•¥å·ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼‰
    import re
    # æ£€æŸ¥åå­—æ˜¯å¦åªåŒ…å«æ ‡ç‚¹ç¬¦å·ã€ç©ºç™½å­—ç¬¦æˆ–ç‰¹æ®Šç¬¦å·ï¼ˆä¸åŒ…å«æ±‰å­—ã€å­—æ¯ã€æ•°å­—ï¼‰
    def is_invalid_name(name):
        # å¦‚æœåå­—ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦ï¼Œç›´æ¥è¿”å› True
        if not name or name.strip() == '':
            return True
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆå­—ç¬¦ï¼ˆæ±‰å­—ã€å­—æ¯ã€æ•°å­—ï¼‰
        if re.search(r'[\u4e00-\u9fa5a-zA-Z0-9]', name):
            return False
        # å¦‚æœä¸åŒ…å«ä»»ä½•æœ‰æ•ˆå­—ç¬¦ï¼Œåˆ™è®¤ä¸ºæ˜¯æ— æ•ˆåå­—ï¼ˆåªåŒ…å«æ ‡ç‚¹ç¬¦å·ç­‰ï¼‰
        return True
    
    auto_err_list = []
    for name in names:
        if is_invalid_name(name):
            auto_err_list.append(name)
    
    # åˆå¹¶æ‰‹åŠ¨é”™è¯¯åˆ—è¡¨å’Œè‡ªåŠ¨æ£€æµ‹çš„é”™è¯¯åˆ—è¡¨
    all_err_list = list(set(err + auto_err_list))
    
    if len(all_err_list) != 0:
        name_new = list(set(names)-set(all_err_list))  # å»é”™åçš„åå­—åˆ—è¡¨
        indexes = [list(names).index(n) for n in name_new]
        names = np.array(name_new)
        rel = rel[indexes, :][:, indexes]
        if len(auto_err_list) > 0:
            print(f"âœ… è‡ªåŠ¨è¿‡æ»¤ï¼šåˆ é™¤äº† {len(auto_err_list)} ä¸ªæ— æ•ˆäººåï¼ˆæ ‡ç‚¹ç¬¦å·ç­‰ï¼‰: {sorted(auto_err_list)}")

    # å»é‡ï¼šå¦‚æœä¸€ä¸ªè¾ƒçŸ­çš„äººåæ˜¯å¦ä¸€ä¸ªæ›´é•¿äººåçš„å­ä¸²ï¼Œåˆ é™¤è¾ƒçŸ­çš„äººå
    # ä¾‹å¦‚ï¼š"è·¯é’æ€œ" å’Œ "è·¯é’" -> ä¿ç•™ "è·¯é’æ€œ"ï¼Œåˆ é™¤ "è·¯é’"
    # ä¾‹å¦‚ï¼š"é¡¾ç§‹ç»µ" å’Œ "é¡¾ç§‹" -> ä¿ç•™ "é¡¾ç§‹ç»µ"ï¼Œåˆ é™¤ "é¡¾ç§‹"
    names_list = names.tolist()
    names_to_remove = set()
    name_frequencies = {name: rel[names_list.index(name), names_list.index(name)] for name in names_list}
    
    # æŒ‰é•¿åº¦æ’åºï¼Œå…ˆå¤„ç†è¾ƒçŸ­çš„åç§°
    sorted_names = sorted(names_list, key=lambda x: (len(x), -name_frequencies.get(x, 0)))
    
    for i, short_name in enumerate(sorted_names):
        if short_name in names_to_remove:
            continue
        
        short_freq = name_frequencies.get(short_name, 0)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´é•¿çš„åå­—åŒ…å«è¿™ä¸ªçŸ­åå­—
        for long_name in names_list:
            if long_name == short_name or long_name in names_to_remove:
                continue
            
            # å¦‚æœçŸ­åå­—æ˜¯é•¿åå­—çš„å­ä¸²ï¼ˆå‰ç¼€ã€åç¼€æˆ–ä¸­é—´éƒ¨åˆ†ï¼‰
            if short_name in long_name and len(short_name) < len(long_name):
                long_freq = name_frequencies.get(long_name, 0)
                
                # å¦‚æœé•¿åå­—é¢‘ç‡æ›´é«˜æˆ–ç›¸å½“ï¼ˆè‡³å°‘æ˜¯çŸ­åå­—çš„ 0.5 å€ï¼‰ï¼Œåˆ™åˆ é™¤çŸ­åå­—
                # è¿™æ ·å¯ä»¥å¤„ç† "è·¯é’æ€œ" å’Œ "è·¯é’" çš„æƒ…å†µ
                if long_freq >= short_freq * 0.5:
                    names_to_remove.add(short_name)
                    # å°†çŸ­åå­—çš„å…³ç³»åˆå¹¶åˆ°é•¿åå­—ä¸­
                    short_idx = names_list.index(short_name)
                    long_idx = names_list.index(long_name)
                    rel[long_idx, :] += rel[short_idx, :]
                    rel[:, long_idx] += rel[:, short_idx]
                    break
    
    # ç§»é™¤éœ€è¦åˆ é™¤çš„åå­—
    if names_to_remove:
        name_new = [n for n in names_list if n not in names_to_remove]
        indexes = [names_list.index(n) for n in name_new]
        names = np.array(name_new)
        rel = rel[indexes, :][:, indexes]
        print(f"âœ… å»é‡å¤„ç†ï¼šåˆ é™¤äº† {len(names_to_remove)} ä¸ªé‡å¤å­ä¸²äººå: {sorted(names_to_remove)}")

    # è¿‡æ»¤æ‰ä½é¢‘çš„åå­—
    if threshold != 0:
        if threshold == -1:
            rel_threshold = max(rel.diagonal().mean(), threshold)
        else:
            rel_threshold = threshold
        print("out threshold:{:.3f}".format(rel_threshold))
        rel_filter = np.diag(rel) > rel_threshold
        names = names[rel_filter]
        rel = rel[rel_filter, :][:, rel_filter]
    

    # äººåæ’åº
    indexes = np.argsort(np.diag(rel))[::-1]  # ä»å¤§åˆ°å°
    names = names[indexes]
    rel = rel[indexes, :][:, indexes]

    # é™åˆ¶æœ€å¤šæ˜¾ç¤º60ä¸ªäººåï¼ˆæŒ‰å‡ºç°é¢‘ç‡ä»é«˜åˆ°ä½ï¼‰
    MAX_NAMES = 60
    original_count = len(names)
    if len(names) > MAX_NAMES:
        names = names[:MAX_NAMES]
        rel = rel[:MAX_NAMES, :][:, :MAX_NAMES]
        print(f"âš ï¸ é™åˆ¶æ˜¾ç¤ºäººæ•°ï¼šä¿ç•™å‰ {MAX_NAMES} ä¸ªé«˜é¢‘äººç‰©ï¼ˆå…± {original_count} ä¸ªï¼‰")

    # æ‰“å°æ‰€æœ‰äººå
    print(f"æ‰€æœ‰äººå: {names}")
    return rel, names


def find_paragraphs_with_two_names(text_lines, names_list, context_lines=3, max_paragraphs_per_person=20):
    """
    æ‰¾å‡ºæ‰€æœ‰è‡³å°‘åŒ…å«ä¸¤ä¸ªåå­—çš„æ®µè½ï¼Œå¹¶æ ¹æ®äººåé™åˆ¶æ®µè½æ•°é‡
    
    Args:
        text_lines: æ–‡æœ¬è¡Œåˆ—è¡¨
        names_list: äººååˆ—è¡¨ï¼ˆåº”è¯¥æ˜¯æœ€ç»ˆè¿‡æ»¤åçš„äººåï¼Œé¿å…å­ä¸²é‡å¤ï¼‰
        context_lines: æ®µè½ä¸Šä¸‹æ–‡è¡Œæ•°ï¼ˆå‰åå„å¤šå°‘è¡Œï¼‰
        max_paragraphs_per_person: æ¯ä¸ªäººåæœ€å¤šä¿ç•™çš„æ®µè½æ•°
    
    Returns:
        paragraphs_data: [(paragraph, line_idx, found_names_list), ...]
    """
    # è¿‡æ»¤æ‰å­ä¸²äººåï¼šå¦‚æœçŸ­åå­—æ˜¯é•¿åå­—çš„å­ä¸²ï¼Œä¸”åœ¨åŒä¸€äººååˆ—è¡¨ä¸­ï¼Œåªä¿ç•™é•¿åå­—
    def filter_substring_names(names):
        """è¿‡æ»¤æ‰æ˜¯å…¶ä»–åå­—å­ä¸²çš„åå­—"""
        names_unique = list(set(names))
        names_sorted = sorted(names_unique, key=len, reverse=True)
        filtered = []
        
        for name in names_sorted:
            # æ£€æŸ¥è¿™ä¸ªåå­—æ˜¯å¦æ˜¯å·²ä¿ç•™åå­—çš„å­ä¸²
            is_substring = False
            for kept_name in filtered:
                if name in kept_name and name != kept_name:
                    is_substring = True
                    break
            if not is_substring:
                filtered.append(name)
        
        return filtered
    
    # è¿‡æ»¤å­ä¸²äººå
    names_filtered = filter_substring_names(names_list)
    print(f"ğŸ“‹ è¿‡æ»¤å­ä¸²äººå: {len(names_list)} -> {len(names_filtered)} ä¸ª")
    if len(names_list) != len(names_filtered):
        removed = set(names_list) - set(names_filtered)
        print(f"   ç§»é™¤çš„å­ä¸²äººå: {sorted(removed)}")
    
    # æ„å»ºäººååŒ¹é…æ¨¡å¼ï¼ˆæŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿åå­—ï¼‰
    names_sorted = sorted(names_filtered, key=len, reverse=True)
    
    # ç¬¬ä¸€éï¼šæ‰¾å‡ºæ‰€æœ‰åŒ…å«è‡³å°‘ä¸¤ä¸ªäººåçš„æ®µè½
    all_paragraphs = []
    
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„å»é‡æ–¹å¼ï¼šå­˜å‚¨æ®µè½å†…å®¹æœ¬èº«ï¼Œè€Œä¸æ˜¯hashï¼ˆhashå¯èƒ½å†²çªï¼‰
    seen_paragraph_texts = set()
    
    # ä¸ºäº†è¿›ä¸€æ­¥å»é‡ï¼Œè®°å½•æ¯ä¸ªæ®µè½çš„å”¯ä¸€æ ‡è¯†ï¼ˆåŸºäºå†…å®¹å’Œè¡Œå·èŒƒå›´ï¼‰
    seen_paragraph_keys = set()
    
    for line_idx in range(len(text_lines)):
        # æå–æ®µè½ä¸Šä¸‹æ–‡
        paragraph = extract_paragraph_context(text_lines, line_idx, context_lines)
        
        # å»é‡æ–¹å¼1ï¼šä½¿ç”¨æ®µè½å†…å®¹æœ¬èº«ä½œä¸ºé”®ï¼ˆé¿å…hashå†²çªï¼‰
        if paragraph in seen_paragraph_texts:
            continue
        
        # å»é‡æ–¹å¼2ï¼šä½¿ç”¨æ®µè½å†…å®¹+è¡Œå·èŒƒå›´ä½œä¸ºå”¯ä¸€é”®ï¼ˆé¿å…ç›¸é‚»è¡Œäº§ç”Ÿçš„é‡å¤æ®µè½ï¼‰
        paragraph_key = (paragraph, line_idx // (context_lines * 2 + 1))  # æŒ‰æ®µè½åŒºåŸŸåˆ†ç»„
        if paragraph_key in seen_paragraph_keys:
            continue
        
        seen_paragraph_texts.add(paragraph)
        seen_paragraph_keys.add(paragraph_key)
        
        # æ‰¾å‡ºæ®µè½ä¸­å‡ºç°çš„æ‰€æœ‰äººåï¼ˆä½¿ç”¨ç²¾ç¡®åŒ¹é…ï¼Œé¿å…å­ä¸²è¯¯åŒ¹é…ï¼‰
        found_names = []
        for name in names_sorted:
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…ï¼šç¡®ä¿æ˜¯å®Œæ•´è¯åŒ¹é…ï¼Œè€Œä¸æ˜¯å­ä¸²åŒ¹é…
            # æ£€æŸ¥ name æ˜¯å¦ä½œä¸ºç‹¬ç«‹è¯å‡ºç°åœ¨æ®µè½ä¸­
            if name in paragraph:
                # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯å…¶ä»–åå­—çš„ä¸€éƒ¨åˆ†ï¼ˆå·²é€šè¿‡æ’åºé¿å…ï¼‰
                found_names.append(name)
        
        # å¦‚æœæ‰¾åˆ°è‡³å°‘ä¸¤ä¸ªäººåï¼Œè®°å½•ä¸‹æ¥
        if len(found_names) >= 2:
            all_paragraphs.append((paragraph, line_idx, found_names))
    
    print(f"âœ… æ‰¾åˆ° {len(all_paragraphs)} ä¸ªåŒ…å«è‡³å°‘ä¸¤ä¸ªäººåçš„æ®µè½")
    
    # ç¬¬äºŒéï¼šæŒ‰äººåé™åˆ¶æ®µè½æ•°é‡ï¼Œæ¯ä¸ªäººæœ€å¤šä¿ç•™ max_paragraphs_per_person ä¸ªæ®µè½
    person_paragraph_count = defaultdict(int)  # ç»Ÿè®¡æ¯ä¸ªäººåå·²ç»ä¿ç•™çš„æ®µè½æ•°
    selected_paragraphs = []
    
    # æŒ‰è¡Œå·æ’åºï¼Œä¿æŒé¡ºåº
    all_paragraphs.sort(key=lambda x: x[1])
    
    for paragraph, line_idx, found_names in all_paragraphs:
        # æ£€æŸ¥è¿™ä¸ªæ®µè½ä¸­æ˜¯å¦è¿˜æœ‰æœªè¾¾åˆ°ä¸Šé™çš„äººå
        can_add = False
        for name in found_names:
            if person_paragraph_count[name] < max_paragraphs_per_person:
                can_add = True
                break
        
        if can_add:
            # æ·»åŠ è¿™ä¸ªæ®µè½ï¼Œå¹¶æ›´æ–°è®¡æ•°
            selected_paragraphs.append((paragraph, line_idx, found_names))
            for name in found_names:
                person_paragraph_count[name] += 1
    
    print(f"âœ… é™åˆ¶åä¿ç•™ {len(selected_paragraphs)} ä¸ªæ®µè½ï¼ˆæ¯ä¸ªäººæœ€å¤š {max_paragraphs_per_person} ä¸ªï¼‰")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š äººåæ®µè½ç»Ÿè®¡ï¼ˆå‰10ä¸ªï¼‰:")
    sorted_persons = sorted(person_paragraph_count.items(), key=lambda x: x[1], reverse=True)
    for name, count in sorted_persons[:10]:
        print(f"   {name}: {count} ä¸ªæ®µè½")
    
    return selected_paragraphs


def extract_paragraph_context(text_lines, sentence_line_idx, context_lines=3):
    """
    æå–å¥å­æ‰€åœ¨çš„æ®µè½ä¸Šä¸‹æ–‡
    
    Args:
        text_lines: æ–‡æœ¬è¡Œåˆ—è¡¨
        sentence_line_idx: å¥å­æ‰€åœ¨çš„è¡Œç´¢å¼•
        context_lines: ä¸Šä¸‹æ–‡è¡Œæ•°ï¼ˆå‰åå„å¤šå°‘è¡Œï¼‰
    
    Returns:
        paragraph: æ®µè½æ–‡æœ¬
    """
    start_idx = max(0, sentence_line_idx - context_lines)
    end_idx = min(len(text_lines), sentence_line_idx + context_lines + 1)
    
    paragraph = '\n'.join(text_lines[start_idx:end_idx])
    return paragraph.strip()


def analyze_relationships_with_llm(text_lines, names_list, base_url, api_key, model_name,
                                   max_sentences=200, context_lines=3):
    """
    ä½¿ç”¨ LLMï¼ˆDeepSeekï¼‰åˆ†æäººç‰©å…³ç³»
    
    Args:
        text_lines: æ–‡æœ¬è¡Œåˆ—è¡¨
        names_list: äººååˆ—è¡¨
        base_url: API åŸºç¡€ URLï¼ˆå¦‚ https://api.deepseek.comï¼‰
        api_key: API å¯†é’¥
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ deepseek-reasoner æˆ– deepseek-chatï¼‰
        max_sentences: æœ€å¤šåˆ†æçš„å¥å­æ•°
        context_lines: æ®µè½ä¸Šä¸‹æ–‡è¡Œæ•°
    
    Returns:
        relationships: [(person1, relation, person2, weight), ...]
        all_names: æ‰€æœ‰äººåé›†åˆ
        paragraphs_data: [(paragraph, line_idx, person1, person2, sentence), ...] æ®µè½æ•°æ®åˆ—è¡¨
    """
    if not OPENAI_AVAILABLE:
        print("âŒ OpenAI åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ LLM åˆ†æ")
        return [], set(names_list), []
    
    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆDeepSeek å…¼å®¹ OpenAI APIï¼‰
    try:
        client = OpenAI(
            api_key=api_key,
            base_url=base_url.rstrip('/')
        )
        print(f"âœ… å·²è¿æ¥åˆ° DeepSeek API: {base_url}")
        print(f"ğŸ“¦ ä½¿ç”¨æ¨¡å‹: {model_name}")
    except Exception as e:
        print(f"âŒ API åˆå§‹åŒ–å¤±è´¥: {e}")
        return [], set(names_list), []
    
    # é˜¶æ®µ1: æ‰¾å‡ºæ‰€æœ‰åŒ…å«è‡³å°‘ä¸¤ä¸ªäººåçš„æ®µè½
    print(f"\nğŸ” é˜¶æ®µ1: æ‰¾å‡ºæ‰€æœ‰åŒ…å«è‡³å°‘ä¸¤ä¸ªäººåçš„æ®µè½...")
    paragraphs_with_names = find_paragraphs_with_two_names(
        text_lines, names_list, context_lines=context_lines, max_paragraphs_per_person=20
    )
    
    if len(paragraphs_with_names) == 0:
        print("âš ï¸ æœªæ‰¾åˆ°åŒ…å«è‡³å°‘ä¸¤ä¸ªäººåçš„æ®µè½")
        return [], set(names_list), []
    
    # å‡†å¤‡æ®µè½æ•°æ®ç”¨äºå¯¼å‡º Excelï¼ˆè¿›ä¸€æ­¥å»é‡ï¼‰
    paragraphs_data_for_excel = []
    unique_paragraphs = []
    
    # ç”¨äºè®°å½•å·²å¯¼å‡ºçš„æ®µè½ï¼Œé¿å…é‡å¤
    exported_paragraphs = set()
    
    for paragraph, line_idx, found_names in paragraphs_with_names:
        unique_paragraphs.append((paragraph, line_idx))
        
        # ä¸º Excel å¯¼å‡ºå‡†å¤‡æ•°æ®ï¼šåˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„äººåå¯¹
        # ä½†æ¯ä¸ªæ®µè½åªå¯¼å‡ºä¸€æ¬¡ï¼ˆåŸºäºæ®µè½å†…å®¹ï¼‰
        paragraph_text = paragraph.strip()
        if paragraph_text in exported_paragraphs:
            continue  # è·³è¿‡é‡å¤æ®µè½
        exported_paragraphs.add(paragraph_text)
        
        # ä¸ºæ¯ä¸ªäººåå¯¹åˆ›å»ºä¸€æ¡è®°å½•
        for i in range(len(found_names)):
            for j in range(i + 1, len(found_names)):
                person1, person2 = found_names[i], found_names[j]
                if person1 != person2:
                    # æå–æ®µè½ä¸­çš„å¥å­ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
                    sentence_pattern = r'[ã€‚ï¼ï¼Ÿï¼›\n]+'
                    sentences = re.split(sentence_pattern, paragraph)
                    # æ‰¾åˆ°åŒ…å«è¿™ä¸¤ä¸ªäººåçš„å¥å­
                    relevant_sentence = ""
                    for sent in sentences:
                        if person1 in sent and person2 in sent:
                            relevant_sentence = sent.strip()
                            break
                    if not relevant_sentence and sentences:
                        relevant_sentence = sentences[0].strip()[:100]  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€å¥
                    
                    paragraphs_data_for_excel.append((paragraph, line_idx, person1, person2, relevant_sentence))
    
    # æš‚æ—¶å…³é—­ LLM åˆ†æï¼Œåªå¯¼å‡ºæ®µè½æ•°æ®
    print(f"\nâš ï¸ LLM åˆ†æå·²æš‚æ—¶å…³é—­ï¼Œä»…å¯¼å‡ºæ®µè½æ•°æ®ç”¨äºæ£€æŸ¥")
    print(f"âœ… å‡†å¤‡å¯¼å‡º {len(paragraphs_data_for_excel)} æ¡æ®µè½è®°å½•åˆ° Excel")
    
    # è¿”å›ç©ºå…³ç³»åˆ—è¡¨ï¼Œä½†ä¿ç•™æ®µè½æ•°æ®
    # LLM åˆ†æå·²æš‚æ—¶å…³é—­ï¼Œåªå¯¼å‡ºæ®µè½æ•°æ®ç”¨äºæ£€æŸ¥
    relationships = []
    all_names = set(names_list)
    
    # LLM åˆ†æä»£ç å·²æš‚æ—¶å…³é—­ï¼Œå¦‚éœ€å¯ç”¨è¯·å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç 
    # æ³¨æ„ï¼šå–æ¶ˆæ³¨é‡Šæ—¶éœ€è¦ç¡®ä¿ä¸‰å¼•å·å­—ç¬¦ä¸²æ­£ç¡®é…å¯¹
    # 
    # # é˜¶æ®µ3: ä½¿ç”¨ LLM åˆ†ææ®µè½
    # print(f"\nğŸ” é˜¶æ®µ3: ä½¿ç”¨ LLM åˆ†ææ®µè½ä¸­çš„äººç‰©å…³ç³»...")
    # 
    # # æ„å»ºæç¤ºè¯æ¨¡æ¿ï¼ˆæ³¨æ„ï¼šä½¿ç”¨åŒèŠ±æ‹¬å· {{ å’Œ }} æ¥è½¬ä¹‰ JSON ç¤ºä¾‹ä¸­çš„èŠ±æ‹¬å·ï¼‰
    # prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´åˆ†æåŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬æ®µè½ä¸­æå–äººç‰©å…³ç³»ã€‚
    # 
    # è¦æ±‚ï¼š
    # 1. è¯†åˆ«æ®µè½ä¸­å‡ºç°çš„æ‰€æœ‰äººç‰©å§“å
    # 2. æå–äººç‰©ä¹‹é—´çš„å…³ç³»ï¼ˆå¦‚ï¼šçˆ¶å­ã€æœ‹å‹ã€æ‹äººã€åŒäº‹ã€æ•Œäººã€å¸ˆç”Ÿã€ä¸»ä»†ã€å…„å¼Ÿã€å§å¦¹ç­‰ï¼‰
    # 3. å¦‚æœå…³ç³»ä¸æ˜ç¡®ï¼Œä½¿ç”¨"ç›¸å…³"ä½œä¸ºå…³ç³»ç±»å‹
    # 4. åªæå–æ˜ç¡®å‡ºç°çš„å…³ç³»ï¼Œä¸è¦æ¨æµ‹
    # 
    # è¾“å‡ºæ ¼å¼ä¸º JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼å¦‚ä¸‹ï¼š
    # {{
    #   "person1": "äººç‰©1",
    #   "relation": "å…³ç³»ç±»å‹",
    #   "person2": "äººç‰©2"
    # }}
    # 
    # æ–‡æœ¬æ®µè½ï¼š
    # {text}
    # 
    # è¯·åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰äººç‰©å…³ç³»ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚"""
    # 
    # # åˆ†æ‰¹å¤„ç†æ®µè½
    # batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªæ®µè½
    # for i in tqdm(range(0, len(unique_paragraphs), batch_size), desc="åˆ†ææ®µè½"):
    #     batch_paragraphs = unique_paragraphs[i:i+batch_size]
    #     
    #     # åˆå¹¶å¤šä¸ªæ®µè½ä¸ºä¸€ä¸ªè¯·æ±‚
    #     combined_text = "\n\n---\n\n".join([p[0] for p in batch_paragraphs])
    #     prompt = prompt_template.format(text=combined_text)
    #     
    #     try:
    #         # è°ƒç”¨ DeepSeek API
    #         response = client.chat.completions.create(
    #             model=model_name,
    #             messages=[
    #                 {"role": "user", "content": prompt}
    #             ],
    #             max_tokens=2000,
    #             temperature=0.3
    #         )
    #         
    #         # è§£æå“åº”ï¼ˆå¤„ç† reasoning_content å­—æ®µï¼‰
    #         message = response.choices[0].message
    #         content = message.content
    #         
    #         # å¦‚æœä½¿ç”¨ deepseek-reasonerï¼Œå¯èƒ½éœ€è¦å¤„ç† reasoning_content
    #         if hasattr(message, 'reasoning_content') and message.reasoning_content:
    #             # åªä½¿ç”¨æœ€ç»ˆçš„ contentï¼Œå¿½ç•¥æ€ç»´é“¾
    #             pass
    #         
    #         # æå– JSON æ•°ç»„
    #         json_match = re.search(r'\[.*\]', content, re.DOTALL)
    #         if json_match:
    #             json_str = json_match.group(0)
    #             try:
    #                 relations = json.loads(json_str)
    #                 for rel in relations:
    #                     if isinstance(rel, dict) and 'person1' in rel and 'person2' in rel:
    #                         person1 = rel['person1'].strip()
    #                         person2 = rel['person2'].strip()
    #                         relation = rel.get('relation', 'ç›¸å…³').strip()
    #                         
    #                         # è¿‡æ»¤æ‰ç©ºåå­—
    #                         if not person1 or not person2:
    #                             continue
    #                         
    #                         # æ·»åŠ äººååˆ°é›†åˆï¼ˆå…è®¸ LLM è¯†åˆ«æ–°çš„äººåï¼‰
    #                         all_names.add(person1)
    #                         all_names.add(person2)
    #                         
    #                         # è®°å½•å…³ç³»ï¼ˆå…è®¸è®°å½•æ‰€æœ‰äººåå…³ç³»ï¼Œä¸é™åˆ¶åœ¨åŸå§‹åˆ—è¡¨ä¸­ï¼‰
    #                         relationships.append((person1, relation, person2, 1.0))
    #             except json.JSONDecodeError as e:
    #                 print(f"âš ï¸ JSON è§£æå¤±è´¥: {e}")
    #                 print(f"   å“åº”å†…å®¹: {content[:200]}")
    #         
    #         # é¿å…è¯·æ±‚è¿‡å¿«
    #         time.sleep(0.5)
    #         
    #     except Exception as e:
    #         print(f"âš ï¸ API è°ƒç”¨å¤±è´¥: {e}")
    #         continue
    # 
    # print(f"âœ… æå–åˆ° {len(relationships)} ä¸ªå…³ç³»")
    
    return relationships, all_names, paragraphs_data_for_excel


def build_relation_matrix_from_llm(relationships, names_list):
    """
    ä» LLM åˆ†æç»“æœæ„å»ºå…³ç³»çŸ©é˜µ
    
    Args:
        relationships: [(person1, relation, person2, weight), ...]
        names_list: æ‰€æœ‰äººååˆ—è¡¨
    
    Returns:
        rel_matrix: å…³ç³»çŸ©é˜µ (numpy array)
        names_array: äººåæ•°ç»„ (numpy array)
    """
    # åˆ›å»ºäººååˆ°ç´¢å¼•çš„æ˜ å°„
    name_to_idx = {name: idx for idx, name in enumerate(names_list)}
    n = len(names_list)
    
    # åˆå§‹åŒ–å…³ç³»çŸ©é˜µ
    rel_matrix = np.zeros((n, n))
    
    # å¡«å……å…³ç³»çŸ©é˜µ
    for person1, relation, person2, weight in relationships:
        if person1 in name_to_idx and person2 in name_to_idx:
            idx1 = name_to_idx[person1]
            idx2 = name_to_idx[person2]
            # å…³ç³»çŸ©é˜µæ˜¯å¯¹ç§°çš„
            rel_matrix[idx1][idx2] = weight
            rel_matrix[idx2][idx1] = weight
    
    # å¯¹è§’çº¿å­˜å‚¨æ¯ä¸ªåå­—çš„å‡ºç°æ¬¡æ•°ï¼ˆç”¨äºæ’åºï¼‰
    for i, name in enumerate(names_list):
        # ç»Ÿè®¡è¯¥åå­—åœ¨å…³ç³»ä¸­çš„å‡ºç°æ¬¡æ•°
        count = sum(1 for r in relationships if r[0] == name or r[2] == name)
        rel_matrix[i][i] = count
    
    return rel_matrix, np.array(names_list)


def export_paragraphs_to_excel(paragraphs_data, file_path, book_name=None):
    """
    å¯¼å‡ºæ‰¾åˆ°çš„æ®µè½åˆ° Excel æ–‡ä»¶
    
    Args:
        paragraphs_data: æ®µè½æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (paragraph, line_idx, person1, person2, sentence)
        file_path: Excel æ–‡ä»¶è·¯å¾„
        book_name: ä¹¦åï¼ˆç”¨äºæ–‡ä»¶åï¼‰
    """
    if not PANDAS_AVAILABLE:
        print("âš ï¸ pandas æœªå®‰è£…ï¼Œè·³è¿‡æ®µè½å¯¼å‡º")
        return
    
    try:
        os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else ".", exist_ok=True)
        
        # æ•´ç†æ®µè½æ•°æ®å¹¶å»é‡
        paragraph_records = []
        seen_paragraphs = set()  # ç”¨äºå»é‡æ®µè½å†…å®¹
        
        for idx, (paragraph, line_idx, person1, person2, sentence) in enumerate(paragraphs_data, 1):
            # ä½¿ç”¨æ®µè½å†…å®¹ä½œä¸ºå”¯ä¸€é”®è¿›è¡Œå»é‡
            paragraph_key = paragraph.strip()
            
            # å¦‚æœæ®µè½å·²å­˜åœ¨ï¼Œåˆå¹¶äººåå¯¹ä¿¡æ¯ï¼ˆä½†ä¸åœ¨Excelä¸­é‡å¤æ˜¾ç¤ºï¼‰
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„æ®µè½
            if paragraph_key in seen_paragraphs:
                continue  # è·³è¿‡é‡å¤æ®µè½
            
            seen_paragraphs.add(paragraph_key)
            
            paragraph_records.append({
                "åºå·": len(paragraph_records) + 1,  # ä½¿ç”¨å®é™…è®°å½•æ•°ï¼Œè€Œä¸æ˜¯åŸå§‹idx
                "è¡Œå·": line_idx + 1,  # è½¬æ¢ä¸º 1-based è¡Œå·
                "äººç‰©1": person1,
                "äººç‰©2": person2,
                "åŒ…å«çš„å¥å­": sentence,
                "æ®µè½å†…å®¹": paragraph,
                "æ®µè½é•¿åº¦": len(paragraph),
                "å¥å­é•¿åº¦": len(sentence)
            })
        
        print(f"ğŸ“Š å»é‡åä¿ç•™ {len(paragraph_records)} æ¡å”¯ä¸€æ®µè½è®°å½•ï¼ˆåŸå§‹ {len(paragraphs_data)} æ¡ï¼‰")
        
        df_paragraphs = pd.DataFrame(paragraph_records)
        
        # å†™å…¥ Excel
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            df_paragraphs.to_excel(writer, sheet_name='æ‰¾åˆ°çš„æ®µè½', index=False)
            
            # è°ƒæ•´åˆ—å®½ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                worksheet = writer.sheets['æ‰¾åˆ°çš„æ®µè½']
                # è®¾ç½®åˆ—å®½
                worksheet.column_dimensions['A'].width = 8   # åºå·
                worksheet.column_dimensions['B'].width = 10  # è¡Œå·
                worksheet.column_dimensions['C'].width = 15  # äººç‰©1
                worksheet.column_dimensions['D'].width = 15  # äººç‰©2
                worksheet.column_dimensions['E'].width = 50  # åŒ…å«çš„å¥å­
                worksheet.column_dimensions['F'].width = 80  # æ®µè½å†…å®¹
                worksheet.column_dimensions['G'].width = 12  # æ®µè½é•¿åº¦
                worksheet.column_dimensions['H'].width = 12  # å¥å­é•¿åº¦
            except Exception:
                pass  # å¦‚æœè°ƒæ•´åˆ—å®½å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ
        
        print(f"âœ… å·²å¯¼å‡ºæ®µè½åˆ° Excel æ–‡ä»¶: {file_path}")
        print(f"   - å…± {len(paragraph_records)} ä¸ªæ®µè½")
    except Exception as e:
        print(f"âš ï¸ æ®µè½ Excel å¯¼å‡ºå¤±è´¥: {e}")


def export_llm_relationships_to_excel(relationships, names_list, file_path, book_name=None):
    """
    å¯¼å‡º LLM åˆ†æçš„å…³ç³»åˆ° Excel æ–‡ä»¶
    
    Args:
        relationships: [(person1, relation, person2, weight), ...]
        names_list: æ‰€æœ‰äººååˆ—è¡¨
        file_path: Excel æ–‡ä»¶è·¯å¾„
        book_name: ä¹¦åï¼ˆç”¨äºæ–‡ä»¶åï¼‰
    """
    if not PANDAS_AVAILABLE:
        print("âš ï¸ pandas æœªå®‰è£…ï¼Œè·³è¿‡ Excel å¯¼å‡º")
        return
    
    try:
        os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else ".", exist_ok=True)
        
        # å…³ç³»è¯¦æƒ…è¡¨
        rel_data = []
        for person1, relation, person2, weight in relationships:
            rel_data.append({
                "äººç‰©1": person1,
                "å…³ç³»": relation,
                "äººç‰©2": person2,
                "æƒé‡": weight
            })
        df_rel = pd.DataFrame(rel_data)
        
        # äººç‰©ç»Ÿè®¡è¡¨
        entity_data = []
        for name in names_list:
            count = sum(1 for r in relationships if r[0] == name or r[2] == name)
            entity_data.append({
                "äººç‰©": name,
                "å…³ç³»æ•°é‡": count
            })
        df_entity = pd.DataFrame(entity_data)
        df_entity = df_entity.sort_values("å…³ç³»æ•°é‡", ascending=False)
        
        # å…³ç³»ç±»å‹ç»Ÿè®¡è¡¨
        rel_type_counts = defaultdict(int)
        for _, relation, _, _ in relationships:
            rel_type_counts[relation] += 1
        rel_type_data = [{"å…³ç³»ç±»å‹": k, "æ•°é‡": v} 
                        for k, v in sorted(rel_type_counts.items(), key=lambda x: x[1], reverse=True)]
        df_rel_type = pd.DataFrame(rel_type_data)
        
        # å†™å…¥ Excel
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            df_rel.to_excel(writer, sheet_name='å…³ç³»è¯¦æƒ…', index=False)
            df_entity.to_excel(writer, sheet_name='äººç‰©ç»Ÿè®¡', index=False)
            df_rel_type.to_excel(writer, sheet_name='å…³ç³»ç±»å‹ç»Ÿè®¡', index=False)
        
        print(f"âœ… å·²å¯¼å‡º Excel æ–‡ä»¶: {file_path}")
    except Exception as e:
        print(f"âš ï¸ Excel å¯¼å‡ºå¤±è´¥: {e}")


def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æˆ–æ›¿æ¢ä¸å…è®¸çš„å­—ç¬¦"""
    import re
    # ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
    # Windows ä¸æ”¯æŒçš„å­—ç¬¦: < > : " / \ | ? *
    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ã€å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ã€ç©ºæ ¼
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    filename = filename.strip(' .')
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆé¿å…è¿‡é•¿ï¼‰
    if len(filename) > 100:
        filename = filename[:100]
    return filename

def plot_rel(relations, names, draw_all=True, balanced=True, verbose=True, save_path=None, book_name=None):

    # å¹³è¡¡åå­—å…³ç³»
    if balanced == True:
        relations =(relations.T+relations)/2
    

    # ç”»å›¾
    G = nx.Graph()

    # å°†æ¯ä¸ªåå­—ï¼Œå’Œåå­—å‡ºç°çš„æ¬¡æ•°åŠ å…¥å›¾
    nums = np.diag(relations)
    for i,name in enumerate(names):
        G.add_node(name, num = nums[i])

    # å°†å…³ç³»åŠ å…¥å›¾
    for i in range(len(names)):
        for j in range(i+1, len(names)):
            if relations[i, j] != 0:
                G.add_edge(names[i], names[j], weight=relations[i, j])

    # åˆ¤æ–­æ˜¯å¦è”é€šå¹¶åˆ‡åˆ†å­å›¾
    max_weight = 0.0
    #### for c in sorted(nx.connected_components(G), key=len, reverse=True):
    #ã€€ç”»å‡ºä¸»è¦å­å›¾
    main_c = max(nx.connected_components(G), key=len)
    sub_G = G.subgraph(main_c)
    sub_nums = np.array([n[1] for n in sub_G.nodes(data="num")])
    sub_weight = np.array([e[2] for e in sub_G.edges(data="weight")])
    if len(sub_weight) != 0:  # æƒé‡å€¼ä¸º 0 åˆ™ä¸éœ€è¦å½’ä¸€åŒ–
        max_weight = max(np.max(sub_weight), max_weight)
        sub_weight = sub_weight*4.5/max_weight

    #ä¸»è¦å­å›¾å¤–å…¶ä»–çš„å›¾
    other_c = set(G.nodes) - main_c

    #æœ€ç»ˆç»“æœä¿¡æ¯
    info = "<<shown-points>>\n{}\n<<dropout-points>>\n{}".format(
        sub_G.nodes(data="num"), G.subgraph(other_c).nodes(data="num"))
    
    if verbose == True:
        print("="*50)
        print("+++++++ æœ€ç»ˆåˆ†æç»“æœ: +++++++")
        print(info)
        print("="*50)

    # æ£€æµ‹æ˜¯å¦åœ¨ CI ç¯å¢ƒæˆ–éœ€è¦ä¿å­˜æ–‡ä»¶
    is_ci = os.getenv('CI') == 'true' or os.getenv('DISPLAY') is None
    save_images = save_path is not None or is_ci
    
    if save_images and save_path is None:
        save_path = "output"
    
    if save_path:
        os.makedirs(save_path, exist_ok=True)
    
    # æ ¹æ®èŠ‚ç‚¹æ•°é‡è°ƒæ•´å‚æ•°ä»¥é¿å…æ ‡ç­¾é‡å 
    num_nodes = len(sub_G.nodes())
    
    # åŠ¨æ€è°ƒæ•´å‚æ•° - å¢å¤§ç”»å¸ƒä»¥å®¹çº³æ›´å¤šå†…å®¹
    if num_nodes > 50:
        # å¤§é‡èŠ‚ç‚¹æ—¶ï¼šå¢å¤§ç”»å¸ƒã€å‡å°å­—ä½“ã€å¢å¤§èŠ‚ç‚¹é—´è·
        figsize = (32, 24)  # ä» (20, 16) å¢å¤§åˆ° (32, 24)
        font_size = 6
        node_size_multiplier = 20
        k_value = 3  # ç”¨äº spring å¸ƒå±€çš„èŠ‚ç‚¹é—´è·
    elif num_nodes > 30:
        # ä¸­ç­‰èŠ‚ç‚¹æ—¶
        figsize = (24, 20)  # ä» (16, 14) å¢å¤§åˆ° (24, 20)
        font_size = 8
        node_size_multiplier = 40
        k_value = 2
    else:
        # å°‘é‡èŠ‚ç‚¹æ—¶
        figsize = (18, 15)  # ä» (12, 10) å¢å¤§åˆ° (18, 15)
        font_size = 10
        node_size_multiplier = 60
        k_value = 1
    
    # è°ƒæ•´èŠ‚ç‚¹å¤§å°ï¼ˆç¡®ä¿æœ€å°å°ºå¯¸ï¼‰
    node_sizes = np.maximum(sub_nums * node_size_multiplier, 50)
    
    #å¤šç§æ–¹å¼å±•ç¤ºç»“æœ
    def spring_layout_func(G):
        return nx.spring_layout(G, k=k_value, iterations=50)
    
    layouts = [
        ("spring", spring_layout_func),
        ("circular", nx.circular_layout),
        ("kamada_kawai", nx.kamada_kawai_layout),
        ("spectral", nx.spectral_layout),
        ("random", nx.random_layout)
    ]
    
    layout_count = len(layouts) if draw_all else 1
    
    for i, (layout_name, layout_func) in enumerate(layouts[:layout_count]):
        try:
            plt.figure(figsize=figsize)
            
            # è®¡ç®—å¸ƒå±€ä½ç½®ï¼ˆæ·»åŠ è¶…æ—¶å¤„ç†ï¼‰
            try:
                pos = layout_func(sub_G)
            except Exception as e:
                print(f"âš ï¸ å¸ƒå±€ç®—æ³• {layout_name} è®¡ç®—å¤±è´¥: {e}")
                print(f"   ä½¿ç”¨ spring å¸ƒå±€ä½œä¸ºå¤‡é€‰")
                pos = spring_layout_func(sub_G)
            
            # ç»˜åˆ¶èŠ‚ç‚¹å’Œè¾¹
            nx.draw_networkx_nodes(sub_G, pos, node_size=node_sizes, node_color='lightblue', 
                                  alpha=0.7, edgecolors='black', linewidths=0.5)
            nx.draw_networkx_edges(sub_G, pos, width=sub_weight, alpha=0.5, edge_color='gray')
            
            # ç»˜åˆ¶æ ‡ç­¾ï¼Œä½¿ç”¨æ›´å¥½çš„å‚æ•°é¿å…é‡å 
            labels = {node: node for node in sub_G.nodes()}
            nx.draw_networkx_labels(sub_G, pos, labels, font_size=font_size, 
                                   font_family='sans-serif', 
                                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                                            edgecolor='none', alpha=0.7))
            
            plt.title(f"äººç‰©å…³ç³»å›¾ - {layout_name} (å…±{num_nodes}ä¸ªäººç‰©)", fontsize=14, pad=20)
            plt.axis('off')
            
            if save_images:
                # ç”Ÿæˆæ–‡ä»¶åï¼ŒåŒ…å«ä¹¦ç±åç§°ï¼ˆå¦‚æœæä¾›ï¼‰
                if book_name:
                    safe_book_name = sanitize_filename(book_name)
                    filename_base = f"{safe_book_name}_relationship_{layout_name}"
                else:
                    filename_base = f"relationship_{layout_name}"
                
                filename = os.path.join(save_path, f"{filename_base}.png") if save_path else f"{filename_base}.png"
                plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white', edgecolor='none')
                if verbose:
                    print(f"âœ… å·²ä¿å­˜å›¾ç‰‡: {filename} (èŠ‚ç‚¹æ•°: {num_nodes})")
            plt.close()
        except Exception as e:
            print(f"âŒ ç”Ÿæˆ {layout_name} å¸ƒå±€å›¾æ—¶å‡ºé”™: {e}")
            if 'plt' in locals():
                plt.close()
            continue
    # nx.draw_shell(sub_G, with_labels=True, node_size=sub_nums, width=sub_weight)
    # plt.show()

def trans_list2dict(trans_list):
    """
    æŠŠåˆ«åè½¬æ¢åˆ—è¡¨è½¬æ¢ä¸ºåˆ«åè½¬æ¢å­—å…¸
    """
    trans_dict = {}
    for names in trans_list:
        for i,name in enumerate(names):
            if i==0:
                continue
            trans_dict[name] = names[0]
    return trans_dict



# ["ç½—è¾‘","ç¨‹å¿ƒ","æ±ªæ·¼","å¶æ–‡æ´","å²å¼º","ç»´å¾·","äº‘å¤©æ˜","å¸Œæ©æ–¯","é›·è¿ªäºšå…¹","ä¸ä»ª","æ³°å‹’","ç« åŒ—æµ·","å…³ä¸€å¸†","æ–‡æ´","åŒ—æµ·","å¤©æ˜","ä¸€å¸†","ä¼Ÿæ€","æ–‡æ–¯","å«å®","å§‹çš‡","å¿ƒè¯´","æ–‡ç‹","ç‰è²","å¿—æˆ","è¥¿é‡Œ","æ™“æ˜","å“²æ³°","åº„é¢œ","å¢¨å­","æ¨æ™‹æ–‡","æ™‹æ–‡","æ…ˆæ¬£","æ²éœ–","å¼ æ´æœ","æ´æœ","è‰¾AA","AA"]
# info = ["æ—é»›ç‰","è–›å®é’—","è´¾å…ƒæ˜¥","è´¾è¿æ˜¥","è´¾æ¢æ˜¥","è´¾æƒœæ˜¥","æçº¨","å¦™ç‰","å²æ¹˜äº‘","ç‹ç†™å‡¤","è´¾å·§å§","ç§¦å¯å¿","æ™´é›¯","éºæœˆ","è¢­äºº","é¸³é¸¯","é›ªé›","ç´«é¹ƒ","ç¢§ç—•","å¹³å„¿","é¦™è±","é‡‘é’","å¸æ£‹","æŠ±ç´","èµ–å¤§","ç„¦å¤§","ç‹å–„ä¿","å‘¨ç‘","æ—ä¹‹å­","ä¹Œè¿›å­","åŒ…å‹‡","å´è´µ","å´æ–°ç™»","é‚“å¥½æ—¶","ç‹æŸ±å„¿","ä½™ä¿¡","åº†å„¿","æ˜­å„¿","å…´å„¿","éš†å„¿","å å„¿","å–œå„¿","å¯¿å„¿","ä¸°å„¿","ä½å„¿","å°èˆå„¿","æåå„¿","ç‰æŸ±å„¿","è´¾æ•¬","è´¾èµ¦","è´¾æ”¿","è´¾å®ç‰","è´¾ç","è´¾ç","è´¾ç¯","è´¾è“‰","è´¾å…°","è´¾èŠ¸","è´¾è”·","è´¾èŠ¹","çªå®˜","èŠ³å®˜","è—•å®˜","è•Šå®˜","è¯å®˜","ç‰å®˜","å®å®˜","é¾„å®˜","èŒ„å®˜","è‰¾å®˜","è±†å®˜","è‘µå®˜","å¦™ç‰","æ™ºèƒ½","æ™ºé€š","æ™ºå–„","åœ†ä¿¡","å¤§è‰²ç©º","å‡€è™š","å½©å±","å½©å„¿","å½©å‡¤","å½©éœ","å½©é¸¾","å½©æ˜","å½©äº‘","è´¾å…ƒæ˜¥","è´¾è¿æ˜¥","è´¾æ¢æ˜¥","è´¾æƒœæ˜¥","è–›èŸ ","è–›èŒ","è–›å®é’—","è–›å®ç´","ç‹å¤«äºº","ç‹ç†™å‡¤","ç‹å­è…¾","ç‹ä»","å°¤è€å¨˜","å°¤æ°","å°¤äºŒå§","å°¤ä¸‰å§","è´¾è“‰","è´¾å…°","è´¾èŠ¸","è´¾èŠ¹","è´¾ç","è´¾ç","è´¾ç¯","è´¾ç‘","è´¾æ•¬","è´¾èµ¦","è´¾æ”¿","è´¾æ•","è´¾ä»£å„’","è´¾ä»£åŒ–","è´¾ä»£ä¿®","è´¾ä»£å–„","æ™´é›¯","é‡‘é’","é¸³é¸¯","å¸æ£‹","è©¹å…‰","å•è˜ä»","ç¨‹æ—¥å…´","ç‹ä½œæ¢…","çŸ³å‘†å­","å¼ å","å†¯æ¸Š","å¼ é‡‘å“¥","èŒ—çƒŸ","æ‰«çº¢","é”„è¯","ä¼´é¹¤","å°é¹Š","å°çº¢","å°è‰","å°èˆå„¿","åˆ˜å§¥å§¥","é©¬é“å©†","å®‹å¬·å¬·","å¼ å¦ˆå¦ˆ","ç§¦é”º","è’‹ç‰è¡","æŸ³æ¹˜è²","ä¸œå¹³ç‹","ä¹Œè¿›å­","å†·å­å…´","å±±å­é‡","æ–¹æ¤¿","è½½æƒ","å¤ç§‰å¿ ","å‘¨å¤ªç›‘","è£˜ä¸–å®‰","æŠ±ç´","å¸æ£‹","ä¾ç”»","å…¥ç”»","çç ","ç¥ç€","ç»ç’ƒ","ç¿¡ç¿ ","å²æ¹˜äº‘","ç¿ ç¼•","ç¬‘å„¿","ç¯†å„¿è´¾æ¢æ˜¥","ä¾ç”»","ç¿ å¢¨","å°è‰","è´¾å®ç‰","èŒ—çƒŸ","è¢­äºº","æ™´é›¯","æ—é»›ç‰","ç´«é¹ƒ","é›ªé›","æ˜¥çº¤","è´¾æƒœæ˜¥","å…¥ç”»","å½©å±","å½©å„¿","è´¾è¿æ˜¥","å½©å‡¤","å½©äº‘","å½©éœ"] 
# hanlp.add(info)
parser = argparse.ArgumentParser(description="æŒ‡å®šä¹¦çš„åå­—")

parser.add_argument("--book", default="weicheng", type=str,
                    help="ä¹¦çš„åå­—ï¼Œä¸å¸¦åç¼€")
parser.add_argument("--debug",default=False,type=bool,help="æ§åˆ¶ä¸­é—´ç»“æœçš„è¾“å‡ºã€‚é»˜è®¤å…³é—­")
# LLM åˆ†æç›¸å…³å‚æ•°ï¼ˆé»˜è®¤ä½¿ç”¨ LLMï¼‰
parser.add_argument("--use_cooccurrence", action="store_true",
                    help="ä½¿ç”¨å…±ç°ç»Ÿè®¡æ–¹æ³•ï¼Œè€Œä¸æ˜¯ LLM åˆ†æï¼ˆé»˜è®¤ä½¿ç”¨ LLMï¼‰")
parser.add_argument("--api_base_url", type=str, default=None,
                    help="API åŸºç¡€ URLï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ API_BASE_URL è¯»å–ï¼Œæˆ–ä½¿ç”¨ https://api.deepseek.comï¼‰")
parser.add_argument("--api_key", type=str, default=None,
                    help="API å¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ API_KEY è¯»å–ï¼‰")
parser.add_argument("--model", type=str, default=None,
                    help="æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ API_MODEL è¯»å–ï¼Œæˆ–ä½¿ç”¨ deepseek-reasonerï¼‰")
parser.add_argument("--max_sentences", type=int, default=200,
                    help="æœ€å¤šåˆ†æçš„å¥å­æ•°ï¼ˆé»˜è®¤ 200ï¼‰")
parser.add_argument("--context_lines", type=int, default=3,
                    help="æ®µè½ä¸Šä¸‹æ–‡è¡Œæ•°ï¼ˆé»˜è®¤ 3ï¼‰")

if __name__ == "__main__":

    # a = str(CustomDictionary.get("é¸¿æ¸"))
    # print(a=="nz 3 ")
    #################################################
    # ############################################# 
    # ############# æ‰‹åŠ¨è°ƒæ•´æ¨¡å‹ ####################
    # å‰æœŸæ·»åŠ çš„å­—å…¸
    name_dict = []
    
    # åæœŸæ•ˆæœä¼˜åŒ–
    trans_list = [] 
    # è½¬æ¢åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹
    # [[name1,name1_,...],[name2,name2_,...],... ]
    # åˆ—è¡¨å†…çš„æ¯ä¸€ä¸ªåˆ—è¡¨ä»£è¡¨ä¸€ä¸ªäººç‰©çš„ä¸€ç»„åˆ«åï¼Œæ‰€æœ‰åˆ«åä¼šè½¬æ¢ä¸ºç¬¬ä¸€ä¸ªåå­—
    
    trans_dict = {}
    trans_dict.update(trans_list2dict(trans_list))

    err_list = []

    threshold = -1
    # ############################################
    # ############################################
    
    # è·å–ä¹¦åå‚æ•°
    args = parser.parse_args()
    fp = "book/"+ args.book +".txt"
    assert os.path.exists(fp),"error!: no such book in "+ fp
    print("=====+++=== NER for book: "+fp+" ===+++=====",flush=True)
    ###################################33
    ###############################
    # æ’å…¥ä¸ªæ€§åŒ–å­—å…¸
    # name_dict = []
    hanlp.add(name_dict)
    #################################
    #################################
    
    # æ„ŸçŸ¥æœºåˆ†æå™¨å¯¹æ–‡æœ¬è¿›è¡Œåˆ†æ
    model = hanlp(custom_dict=True)
    rels, ns, nr_nrf_dict = count_names(fp, model)
    if args.debug:
        f = np.diag(rels) >= 40
        print("="*50)
        print("<<ç²—æå–ç»“æœ>>\nåå­—æ€»æ•°: {} \n{}{}".format(len(ns),ns[f],np.diag(rels)[f]))
        print("="*50)

    ## åˆ†åˆ«ç”Ÿæˆæ–°çš„åç§°å­—å…¸ï¼Œä»¥åŠè½¬æ¢å­—å…¸
    # print(filter_nr(nr_nrf_dict))
    auto_name_list, auto_trans_dict = filter_nr(nr_nrf_dict,first=True)
    if args.debug:
        print("="*50)
        print("<<è‡ªåŠ¨ç”Ÿæˆçš„åç§°åˆ—è¡¨å’Œåç§°è½¬æ¢å­—å…¸>>")
        print("åç§°åˆ—è¡¨:\n", auto_name_list)
        print("åç§°è½¬æ¢å­—å…¸\n",auto_trans_dict)
        print("="*50)
    hanlp.add(auto_name_list)
    
          
    ############################################
    # æ‰‹åŠ¨è°ƒæ•´çš„è½¬æ¢å­—å…¸
    auto_trans_dict.update(trans_dict)
    trans_dict = auto_trans_dict
    ###############################################
    
    
    # é»˜è®¤ä½¿ç”¨ LLM åˆ†æï¼Œé™¤éæ˜ç¡®æŒ‡å®šä½¿ç”¨å…±ç°ç»Ÿè®¡
    use_llm = not args.use_cooccurrence
    
    # å…ˆè¿›è¡Œå…±ç°ç»Ÿè®¡ï¼Œè·å–æœ€ç»ˆè¿‡æ»¤åçš„äººååˆ—è¡¨ï¼ˆç”¨äºæ®µè½æŸ¥æ‰¾ï¼‰
    print(f"\n{'='*60}")
    print(f"ç¬¬ä¸€æ­¥ï¼šå…±ç°ç»Ÿè®¡ï¼ˆè·å–æœ€ç»ˆäººååˆ—è¡¨ï¼‰")
    print(f"{'='*60}")
    
    ### é‡æ–°è¿›è¡Œç»Ÿè®¡å’Œè®¡æ•°ï¼ˆä½¿ç”¨å·²æ·»åŠ çš„å­—å…¸ï¼‰
    model = hanlp(custom_dict=True)#,analyzer="CRF")
    rels,ns,_ = count_names(fp,model)
  
    ##### æ ¹æ®æ‰‹å·¥è°ƒæ•´ä»¥ä¸åŒæ•ˆæœå±•ç¤º
    relations_cooccurrence, names_cooccurrence = filter_names(
            rels, ns, trans=trans_dict, err=err_list, threshold=threshold)
    
    # è·å–æœ€ç»ˆçš„äººååˆ—è¡¨ï¼ˆç”¨äºæ®µè½æŸ¥æ‰¾ï¼‰
    final_names_list = list(names_cooccurrence)
    
    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯äººåçš„è¯
    def filter_non_person_names(names):
        """è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯äººåçš„è¯"""
        # æ˜æ˜¾ä¸æ˜¯äººåçš„è¯åˆ—è¡¨
        exclude_words = {
            'é—»è¨€', 'æŠ«è¨', 'ç¦å…‹æ–¯', 'ç‹',  # æ˜æ˜¾ä¸æ˜¯äººå
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€',  # å¸¸è§è¯
            'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¯ä»¥', 'ä¸èƒ½'
        }
        filtered = [name for name in names if name not in exclude_words]
        return filtered
    
    # è¿‡æ»¤éäººå
    final_names_list = filter_non_person_names(final_names_list)
    print(f"\nâœ… å…±ç°ç»Ÿè®¡å®Œæˆï¼Œå¾—åˆ° {len(final_names_list)} ä¸ªæœ€ç»ˆäººåï¼ˆå·²è¿‡æ»¤éäººåï¼‰")
    print(f"   äººååˆ—è¡¨: {final_names_list}")
    if len(names_cooccurrence) != len(final_names_list):
        removed = set(names_cooccurrence) - set(final_names_list)
        print(f"   å·²æ’é™¤çš„éäººå: {sorted(removed)}")
    
    if use_llm:
        # ä½¿ç”¨ LLM åˆ†æï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
        api_key = args.api_key or os.getenv('API_KEY')
        api_base_url = args.api_base_url or os.getenv('API_BASE_URL', 'https://api.deepseek.com')
        model_name = args.model or os.getenv('API_MODEL', 'deepseek-reasoner')
        
        if not api_key:
            print("âš ï¸ è­¦å‘Š: æœªæä¾› API å¯†é’¥ï¼Œæ— æ³•ä½¿ç”¨ LLM åˆ†æ")
            print("   å›é€€åˆ°å…±ç°ç»Ÿè®¡æ–¹æ³•")
            print("   æç¤º: è®¾ç½®ç¯å¢ƒå˜é‡ API_KEY æˆ–ä½¿ç”¨ --api_key å‚æ•°ä»¥å¯ç”¨ LLM åˆ†æ")
            use_llm = False
        
        if use_llm:
            print(f"\n{'='*60}")
            print(f"ç¬¬äºŒæ­¥ï¼šLLM åˆ†ææ¨¡å¼ï¼ˆä½¿ç”¨æœ€ç»ˆäººååˆ—è¡¨ï¼‰")
            print(f"{'='*60}")
        
            # è¯»å–æ–‡æœ¬æ–‡ä»¶
            try:
                with open(fp, "r", encoding="utf-8") as f:
                    text_lines = [line.strip() for line in f.readlines() if line.strip()]
            except UnicodeDecodeError:
                with open(fp, "r", encoding="gbk") as f:
                    text_lines = [line.strip() for line in f.readlines() if line.strip()]
            
            print(f"ğŸ“– æ–‡æœ¬æ–‡ä»¶: {fp}")
            print(f"ğŸ“ æ€»å…±æœ‰ {len(text_lines)} è¡Œæ–‡æœ¬")
            print(f"ğŸ“¡ API åœ°å€: {api_base_url}")
            print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
            
            # ä½¿ç”¨æœ€ç»ˆè¿‡æ»¤åçš„äººååˆ—è¡¨è¿›è¡Œ LLM åˆ†æï¼ˆè€Œä¸æ˜¯36ä¸ªé«˜é¢‘äººåï¼‰
            print(f"\nğŸ“‹ ä½¿ç”¨å…±ç°ç»Ÿè®¡è¿‡æ»¤åçš„ {len(final_names_list)} ä¸ªæœ€ç»ˆäººåè¿›è¡Œæ®µè½æŸ¥æ‰¾")
            
            # è°ƒç”¨ LLM åˆ†æå‡½æ•°
            relationships, all_names, paragraphs_data = analyze_relationships_with_llm(
                text_lines,
                final_names_list,  # ä½¿ç”¨æœ€ç»ˆè¿‡æ»¤åçš„äººååˆ—è¡¨
                base_url=api_base_url,
                api_key=api_key,
                model_name=model_name,
                max_sentences=args.max_sentences,
                context_lines=args.context_lines
            )
            
            # å¯¼å‡ºæ®µè½æ•°æ®ï¼ˆæ— è®ºæ˜¯å¦æœ‰å…³ç³»ï¼Œå› ä¸ºLLMå·²å…³é—­ï¼‰
            if PANDAS_AVAILABLE and paragraphs_data:
                output_dir = "output"
                os.makedirs(output_dir, exist_ok=True)
                paragraphs_excel_path = os.path.join(output_dir, f"{sanitize_filename(args.book)}_æ‰¾åˆ°çš„æ®µè½.xlsx")
                export_paragraphs_to_excel(paragraphs_data, paragraphs_excel_path, args.book)
            
            if len(relationships) == 0:
                print("âš ï¸ LLM æœªæå–åˆ°ä»»ä½•å…³ç³»ï¼ˆLLM åˆ†æå·²å…³é—­ï¼‰")
                print("ğŸ’¡ æ®µè½æ•°æ®å·²å¯¼å‡ºåˆ° Excelï¼Œè¯·æ£€æŸ¥å†…å®¹æ˜¯å¦æ­£ç¡®")
                use_llm = False
                # å›é€€åˆ°å…±ç°ç»Ÿè®¡ç»“æœ
                relations = relations_cooccurrence
                names = names_cooccurrence
            else:
                # æ„å»ºå…³ç³»çŸ©é˜µ
                # åˆå¹¶æ‰€æœ‰åå­—ï¼Œä¼˜å…ˆä½¿ç”¨ final_names_list ä¸­çš„é¡ºåº
                all_names_list = list(all_names)
                # å…ˆæŒ‰ final_names_list çš„é¡ºåºæ’åºï¼Œç„¶ååŠ ä¸Šä¸åœ¨åˆ—è¡¨ä¸­çš„åå­—
                names_in_list = [name for name in final_names_list if name in all_names_list]
                names_not_in_list = [name for name in all_names_list if name not in final_names_list]
                names_list_sorted = names_in_list + names_not_in_list
                
                relations, names = build_relation_matrix_from_llm(relationships, names_list_sorted)
                
                print(f"\nâœ… LLM åˆ†æå®Œæˆï¼Œæå–åˆ° {len(relationships)} ä¸ªå…³ç³»")
                
                # å¯¼å‡ºå…³ç³»æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if PANDAS_AVAILABLE:
                    output_dir = "output"
                    os.makedirs(output_dir, exist_ok=True)
                    excel_path = os.path.join(output_dir, f"{sanitize_filename(args.book)}_äººç‰©å…³ç³»_LLM.xlsx")
                    export_llm_relationships_to_excel(relationships, names_list_sorted, excel_path, args.book)
    
    if not use_llm:
        # ä½¿ç”¨åŸæœ‰çš„å…±ç°ç»Ÿè®¡æ–¹æ³•
        print(f"\n{'='*60}")
        print(f"ä½¿ç”¨å…±ç°ç»Ÿè®¡æ¨¡å¼")
        print(f"{'='*60}")
        
        # ä½¿ç”¨ä¹‹å‰å·²ç»è®¡ç®—å¥½çš„ç»“æœ
        relations = relations_cooccurrence
        names = names_cooccurrence

    ##### å±•ç¤ºæœ€ç»ˆç»“æœå’Œä¿¡æ¯
    # ä¼ é€’ä¹¦ç±åç§°ç»™ plot_rel å‡½æ•°ï¼Œç”¨äºç”Ÿæˆå¸¦ä¹¦ç±åçš„æ–‡ä»¶å
    plot_rel(relations, names, book_name=args.book)

   
