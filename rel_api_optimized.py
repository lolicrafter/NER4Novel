# encoding=utf-8
# authorï¼š s0mE
# subjectï¼š ä½¿ç”¨åœ¨çº¿ API è¿›è¡Œäººç‰©å…³ç³»æå–ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸¤é˜¶æ®µç­–ç•¥ï¼‰
# dateï¼š 2024
import argparse
import os
import re
import json
import time
from collections import defaultdict
from tqdm import tqdm
import numpy as np
import networkx as nx
import matplotlib
# åœ¨ CI ç¯å¢ƒä¸­ä½¿ç”¨éäº¤äº’å¼åç«¯
if os.getenv('CI') == 'true' or os.getenv('DISPLAY') is None:
    matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib.font_manager as fm

chinese_fonts = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
                 'Noto Sans CJK SC', 'Source Han Sans CN', 'Droid Sans Fallback', 'DejaVu Sans']
available_fonts = [f.name for f in fm.fontManager.ttflist]

font_found = None
for font in chinese_fonts:
    if font in available_fonts:
        font_found = font
        break

if font_found:
    plt.rcParams["font.sans-serif"] = [font_found] + plt.rcParams["font.sans-serif"]
else:
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"] + plt.rcParams["font.sans-serif"]
    print("âš ï¸ Warning: No Chinese font found, Chinese characters may display as squares")

plt.rcParams["axes.unicode_minus"] = False

# å°è¯•å¯¼å…¥ rel.py ä¸­çš„äººåè¯†åˆ«æ¨¡å—
try:
    # å¯¼å…¥ rel.py ä¸­çš„ hanlp ç±»å’Œ count_names å‡½æ•°
    import sys
    import importlib.util
    
    # åŠ¨æ€å¯¼å…¥ rel.py æ¨¡å—
    spec = importlib.util.spec_from_file_location("rel_module", "rel.py")
    rel_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(rel_module)
    
    HANLP_AVAILABLE = True
    print("âœ… å·²åŠ è½½ rel.py ä¸­çš„äººåè¯†åˆ«æ¨¡å—")
except Exception as e:
    HANLP_AVAILABLE = False
    print(f"âš ï¸ æ— æ³•åŠ è½½ rel.py æ¨¡å—ï¼Œå°†ä½¿ç”¨ç®€å•çš„äººåè¯†åˆ«æ–¹æ³•: {e}")


class LLMAPI:
    """ç»Ÿä¸€çš„ LLM API æ¥å£ï¼ˆOpenAI æ ¼å¼ï¼‰"""
    
    def __init__(self, base_url, api_key, model_name):
        """
        Args:
            base_url: API åŸºç¡€ URLï¼ˆå¦‚ https://miaodi.zeabur.appï¼‰
            api_key: API å¯†é’¥
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ deepseek-ai/DeepSeek-V3-0324ï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key or os.getenv('API_KEY')
        self.model_name = model_name
        
        if not self.api_key:
            raise ValueError("éœ€è¦è®¾ç½® API å¯†é’¥ï¼Œé€šè¿‡å‚æ•°æˆ–ç¯å¢ƒå˜é‡ API_KEY")
        
        # æ£€æŸ¥ base_url æ˜¯å¦åŒ…å« /v1/chat/completions
        if '/v1/chat/completions' in self.base_url:
            self.endpoint = self.base_url
        else:
            self.endpoint = f"{self.base_url}/v1/chat/completions"
    
    def call_api(self, prompt, max_tokens=2000, temperature=0.3):
        """è°ƒç”¨ API"""
        import requests
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': temperature,
            'max_tokens': max_tokens
        }
        
        try:
            response = requests.post(self.endpoint, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            result = response.json()
            
            # è§£æå“åº”ï¼ˆOpenAI æ ¼å¼ï¼‰
            return result.get('choices', [{}])[0].get('message', {}).get('content', '')
        except requests.exceptions.RequestException as e:
            print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   å“åº”å†…å®¹: {e.response.text[:500]}")
            raise


def extract_names_with_hanlp(file_path):
    """
    ä½¿ç”¨ rel.py ä¸­çš„ HanLP æ–¹æ³•æå–äººå
    
    Args:
        file_path: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
    
    Returns:
        names_list: äººååˆ—è¡¨
        nr_nrf_dict: äººåç»Ÿè®¡å­—å…¸
    """
    if not HANLP_AVAILABLE:
        return None, None
    
    try:
        # ä½¿ç”¨ rel.py ä¸­çš„æ–¹æ³•
        model = rel_module.hanlp(custom_dict=True)
        _, names, nr_nrf_dict = rel_module.count_names(file_path, model)
        
        # ä½¿ç”¨ filter_nr è·å–é«˜é¢‘å¯ä¿¡åç§°ï¼ˆè¿™æ˜¯å…³é”®æ­¥éª¤ï¼‰
        try:
            # filter_nr ä¼šæ ¹æ®é˜ˆå€¼è‡ªåŠ¨è¿‡æ»¤ï¼Œåªè¿”å›é«˜é¢‘åå­—
            auto_name_list, _ = rel_module.filter_nr(nr_nrf_dict, threshold=-1, first=False)
            # ä¼˜å…ˆä½¿ç”¨ filter_nr è¿”å›çš„é«˜é¢‘åå­—åˆ—è¡¨
            names_list = auto_name_list
            print(f"âœ… ä½¿ç”¨ filter_nr è¿‡æ»¤åå¾—åˆ° {len(names_list)} ä¸ªé«˜é¢‘äººå")
        except Exception as e:
            # å¦‚æœè¿‡æ»¤å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åå­—åˆ—è¡¨ï¼ˆä½†ä¹Ÿè¦æŒ‰é¢‘ç‡æ’åºï¼‰
            print(f"âš ï¸ filter_nr å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åå­—åˆ—è¡¨: {e}")
            names_list = list(names)
            # æŒ‰å‡ºç°é¢‘ç‡æ’åº
            if nr_nrf_dict:
                name_counts = {}
                for name in names_list:
                    count = nr_nrf_dict.get("nr", {}).get(name, 0) + nr_nrf_dict.get("nrf", {}).get(name, 0)
                    name_counts[name] = count
                names_list = sorted(names_list, key=lambda x: name_counts.get(x, 0), reverse=True)
        
        print(f"âœ… ä½¿ç”¨ HanLP æå–åˆ° {len(names_list)} ä¸ªäººåï¼ˆé«˜é¢‘ï¼‰")
        return names_list, nr_nrf_dict
    except Exception as e:
        print(f"âš ï¸ HanLP æå–å¤±è´¥: {e}")
        return None, None


def extract_names_simple(text, min_name_length=2):
    """
    ç®€å•çš„äººåè¯†åˆ«ï¼ˆåŸºäºå¸¸è§æ¨¡å¼ï¼‰
    åœ¨ GitHub Actions ä¸­ï¼Œå¦‚æœ HanLP ä¸å¯ç”¨ï¼Œä½¿ç”¨è¿™ä¸ªç®€å•æ–¹æ³•ä½œä¸ºåå¤‡
    """
    # å¸¸è§çš„ä¸­æ–‡å§“æ°
    surnames = ['å¼ ', 'ç‹', 'æ', 'èµµ', 'åˆ˜', 'é™ˆ', 'æ¨', 'é»„', 'å‘¨', 'å´', 'å¾', 'å­™', 
                 'é©¬', 'æœ±', 'èƒ¡', 'æ—', 'éƒ­', 'ä½•', 'é«˜', 'ç½—', 'éƒ‘', 'æ¢', 'è°¢', 'å®‹',
                 'å”', 'è®¸', 'éŸ©', 'å†¯', 'é‚“', 'æ›¹', 'å½­', 'æ›¾', 'è‚–', 'ç”°', 'è‘£', 'è¢',
                 'æ½˜', 'äº', 'è’‹', 'è”¡', 'ä½™', 'æœ', 'å¶', 'ç¨‹', 'è‹', 'é­', 'å•', 'ä¸',
                 'ä»»', 'æ²ˆ', 'å§š', 'å¢', 'å§œ', 'å´”', 'é’Ÿ', 'è°­', 'é™†', 'æ±ª', 'èŒƒ', 'é‡‘',
                 'çŸ³', 'å»–', 'è´¾', 'å¤', 'éŸ¦', 'ä»˜', 'æ–¹', 'ç™½', 'é‚¹', 'å­Ÿ', 'ç†Š', 'ç§¦',
                 'é‚±', 'æ±Ÿ', 'å°¹', 'è–›', 'é—«', 'æ®µ', 'é›·', 'ä¾¯', 'é¾™', 'å²', 'é™¶', 'é»',
                 'è´º', 'é¡¾', 'æ¯›', 'éƒ', 'é¾š', 'é‚µ', 'ä¸‡', 'é’±', 'ä¸¥', 'è¦ƒ', 'æ­¦', 'æˆ´',
                 'è«', 'å­”', 'å‘', 'æ±¤', 'å¸¸', 'è·¯']
    
    names = set()
    # æŸ¥æ‰¾ 2-4 å­—çš„ä¸­æ–‡å§“åæ¨¡å¼
    # æ¨¡å¼ï¼šå§“æ° + 1-3ä¸ªæ±‰å­—
    pattern = r'([' + ''.join(surnames) + r'][' + '\u4e00-\u9fa5' + r']{1,3})'
    matches = re.findall(pattern, text)
    names.update(matches)
    
    # ä¹ŸæŸ¥æ‰¾è¿ç»­çš„ä¸­æ–‡åå­—ï¼ˆ2-4å­—ï¼‰
    pattern2 = r'[\u4e00-\u9fa5]{2,4}'
    matches2 = re.findall(pattern2, text)
    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯åå­—çš„è¯
    exclude_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€',
                     'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€',
                     'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™æ ·', 'é‚£æ ·'}
    for match in matches2:
        if match not in exclude_words and len(match) >= min_name_length:
            names.add(match)
    
    return list(names)


def find_sentences_with_two_names(text_lines, names_list, max_sentences=200):
    """
    æ‰¾å‡ºåŒ…å«è‡³å°‘ä¸¤ä¸ªäººåçš„å¥å­
    
    Args:
        text_lines: æ–‡æœ¬è¡Œåˆ—è¡¨
        names_list: äººååˆ—è¡¨
        max_sentences: æœ€å¤šè¿”å›çš„å¥å­æ•°
    
    Returns:
        sentences: [(sentence, person1, person2, line_index), ...]
    """
    # æ„å»ºäººååŒ¹é…æ¨¡å¼ï¼ˆæŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿åå­—ï¼‰
    names_sorted = sorted(set(names_list), key=len, reverse=True)
    name_pattern = '|'.join(re.escape(name) for name in names_sorted if len(name) >= 2)
    
    if not name_pattern:
        return []
    
    sentences = []
    sentence_pattern = r'[ã€‚ï¼ï¼Ÿï¼›\n]+'
    
    for line_idx, line in enumerate(text_lines):
        # æŒ‰å¥å­åˆ†å‰²
        line_sentences = re.split(sentence_pattern, line)
        
        for sentence in line_sentences:
            sentence = sentence.strip()
            if len(sentence) < 5:  # è·³è¿‡å¤ªçŸ­çš„å¥å­
                continue
            
            # æ‰¾å‡ºå¥å­ä¸­å‡ºç°çš„æ‰€æœ‰äººå
            found_names = []
            for name in names_sorted:
                if name in sentence:
                    found_names.append(name)
            
            # å¦‚æœæ‰¾åˆ°è‡³å°‘ä¸¤ä¸ªäººåï¼Œè®°å½•ä¸‹æ¥
            if len(found_names) >= 2:
                # è®°å½•æ‰€æœ‰å¯èƒ½çš„äººåå¯¹
                for i in range(len(found_names)):
                    for j in range(i + 1, len(found_names)):
                        person1, person2 = found_names[i], found_names[j]
                        if person1 != person2:
                            sentences.append((sentence, person1, person2, line_idx))
                            
                            if len(sentences) >= max_sentences:
                                return sentences[:max_sentences]
    
    return sentences


def extract_paragraph_context(text_lines, sentence_line_idx, context_lines=3):
    """
    æå–å¥å­æ‰€åœ¨çš„æ®µè½ä¸Šä¸‹æ–‡
    
    Args:
        text_lines: æ–‡æœ¬è¡Œåˆ—è¡¨
        sentence_line_idx: å¥å­æ‰€åœ¨çš„è¡Œç´¢å¼•
        context_lines: ä¸Šä¸‹æ–‡è¡Œæ•°ï¼ˆå‰åå„å¤šå°‘è¡Œï¼‰
    
    Returns:
        paragraph: æ®µè½æ–‡æœ¬
    """
    start_idx = max(0, sentence_line_idx - context_lines)
    end_idx = min(len(text_lines), sentence_line_idx + context_lines + 1)
    
    paragraph = '\n'.join(text_lines[start_idx:end_idx])
    return paragraph.strip()


def extract_relationships_optimized(file_path, base_url, api_key, model_name,
                                   max_sentences=200, context_lines=3):
    """
    ä¼˜åŒ–çš„ä¸¤é˜¶æ®µå…³ç³»æå–ç­–ç•¥
    
    é˜¶æ®µ1: ä½¿ç”¨ç®€å•æ–¹æ³•æ‰¾å‡ºåŒ…å«ä¸¤ä¸ªäººç‰©çš„å¥å­
    é˜¶æ®µ2: æå–è¿™äº›å¥å­çš„æ®µè½ä¸Šä¸‹æ–‡
    é˜¶æ®µ3: ä½¿ç”¨ LLM åˆ†ææ®µè½ä¸­çš„äººç‰©å…³ç³»
    """
    # åˆå§‹åŒ– API
    try:
        llm = LLMAPI(base_url=base_url, api_key=api_key, model_name=model_name)
        print(f"âœ… å·²è¿æ¥åˆ° API: {base_url}")
        print(f"ğŸ“¦ ä½¿ç”¨æ¨¡å‹: {model_name}")
    except Exception as e:
        print(f"âŒ API åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    print("ğŸ“– æ­£åœ¨è¯»å–æ–‡æœ¬æ–‡ä»¶...")
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
    except UnicodeDecodeError:
        with open(file_path, "r", encoding="gbk") as f:
            lines = f.readlines()
    
    # æ¸…ç†æ–‡æœ¬è¡Œ
    text_lines = [line.strip() for line in lines if line.strip()]
    
    print(f"ğŸ“ æ€»å…±æœ‰ {len(text_lines)} è¡Œæ–‡æœ¬")
    
    # é˜¶æ®µ1: æå–æ‰€æœ‰äººåï¼ˆä¼˜å…ˆä½¿ç”¨ rel.py ä¸­çš„ HanLP æ–¹æ³•ï¼‰
    print("\nğŸ” é˜¶æ®µ1: æå–æ–‡æœ¬ä¸­çš„äººå...")
    
    # å°è¯•ä½¿ç”¨ rel.py ä¸­çš„ HanLP æ–¹æ³•
    names_list, nr_nrf_dict = extract_names_with_hanlp(file_path)
    
    if names_list is None:
        # å¦‚æœ HanLP ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•
        print("âš ï¸ ä½¿ç”¨ç®€å•çš„äººåè¯†åˆ«æ–¹æ³•ï¼ˆHanLP ä¸å¯ç”¨ï¼‰")
        all_text = '\n'.join(text_lines)
        names_list = extract_names_simple(all_text)
        
        # è¿‡æ»¤å’Œå»é‡
        exclude_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€',
                         'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™æ ·', 'é‚£æ ·', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¯ä»¥',
                         'ä¸èƒ½', 'ä¸ä¼š', 'æ²¡æœ‰', 'ä¸æ˜¯', 'ä¹Ÿè¦', 'è¿˜è¦', 'è¿˜è¦', 'è¿˜è¦'}
        names_list = [name for name in names_list 
                      if len(name) >= 2 and name not in exclude_words]
        
        # ç»Ÿè®¡åå­—å‡ºç°é¢‘ç‡ï¼Œåªä¿ç•™é«˜é¢‘åå­—
        name_counts = defaultdict(int)
        for name in names_list:
            name_counts[name] += all_text.count(name)
        
        # ä¿ç•™å‡ºç°è‡³å°‘3æ¬¡çš„åå­—
        names_list = [name for name in names_list if name_counts[name] >= 3]
        names_list = sorted(set(names_list), key=lambda x: name_counts[x], reverse=True)
    
    # è¿‡æ»¤æ‰æ— æ•ˆåå­—ï¼ˆåªåŒ…å«æ ‡ç‚¹ç¬¦å·ç­‰ï¼‰
    def is_invalid_name(name):
        if not name or name.strip() == '':
            return True
        if re.search(r'[\u4e00-\u9fa5a-zA-Z0-9]', name):
            return False
        return True
    
    names_list = [name for name in names_list if not is_invalid_name(name)]
    
    # é™åˆ¶åå­—æ•°é‡ï¼ˆæœ€å¤š50ä¸ªï¼‰
    names_list = names_list[:50]
    
    print(f"âœ… æ‰¾åˆ° {len(names_list)} ä¸ªäººå")
    if len(names_list) > 0:
        print(f"   å‰10ä¸ª: {names_list[:10]}")
    
    if len(names_list) < 2:
        print("âš ï¸ äººåå¤ªå°‘ï¼Œæ— æ³•æå–å…³ç³»")
        return [], []
    
    # é˜¶æ®µ2: æ‰¾å‡ºåŒ…å«ä¸¤ä¸ªäººåçš„å¥å­
    print(f"\nğŸ” é˜¶æ®µ2: æ‰¾å‡ºåŒ…å«è‡³å°‘ä¸¤ä¸ªäººåçš„å¥å­ï¼ˆæœ€å¤š {max_sentences} ä¸ªï¼‰...")
    sentences_with_names = find_sentences_with_two_names(
        text_lines, names_list, max_sentences=max_sentences
    )
    
    print(f"âœ… æ‰¾åˆ° {len(sentences_with_names)} ä¸ªåŒ…å«ä¸¤ä¸ªäººåçš„å¥å­")
    
    if len(sentences_with_names) == 0:
        print("âš ï¸ æœªæ‰¾åˆ°åŒ…å«ä¸¤ä¸ªäººåçš„å¥å­")
        return [], names_list
    
    # é˜¶æ®µ3: æå–æ®µè½å¹¶åˆ†æ
    print(f"\nğŸ” é˜¶æ®µ3: æå–æ®µè½ä¸Šä¸‹æ–‡å¹¶ä½¿ç”¨ LLM åˆ†æå…³ç³»...")
    
    # å»é‡ï¼šåŒä¸€ä¸ªå¥å­å¯èƒ½æœ‰å¤šä¸ªäººåå¯¹
    seen_sentences = set()
    unique_sentences = []
    for sentence, p1, p2, line_idx in sentences_with_names:
        sentence_key = (sentence, line_idx)
        if sentence_key not in seen_sentences:
            seen_sentences.add(sentence_key)
            unique_sentences.append((sentence, p1, p2, line_idx))
    
    print(f"âœ… å»é‡åå…±æœ‰ {len(unique_sentences)} ä¸ªå”¯ä¸€æ®µè½")
    
    relationships = []
    entities = set(names_list)
    
    # æ„å»ºæç¤ºè¯æ¨¡æ¿
    prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´åˆ†æåŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬æ®µè½ä¸­æå–äººç‰©å…³ç³»ã€‚

è¦æ±‚ï¼š
1. è¯†åˆ«æ®µè½ä¸­å‡ºç°çš„æ‰€æœ‰äººç‰©å§“å
2. æå–äººç‰©ä¹‹é—´çš„å…³ç³»ï¼ˆå¦‚ï¼šçˆ¶å­ã€æœ‹å‹ã€æ‹äººã€åŒäº‹ã€æ•Œäººã€å¸ˆç”Ÿã€ä¸»ä»†ã€å…„å¼Ÿã€å§å¦¹ç­‰ï¼‰
3. å¦‚æœå…³ç³»ä¸æ˜ç¡®ï¼Œä½¿ç”¨"ç›¸å…³"ä½œä¸ºå…³ç³»ç±»å‹
4. åªæå–æ˜ç¡®å‡ºç°çš„å…³ç³»ï¼Œä¸è¦æ¨æµ‹

è¾“å‡ºæ ¼å¼ä¸º JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼å¦‚ä¸‹ï¼š
{
  "person1": "äººç‰©1",
  "relation": "å…³ç³»ç±»å‹",
  "person2": "äººç‰©2"
}

æ–‡æœ¬æ®µè½ï¼š
{text}

è¯·åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰äººç‰©å…³ç³»ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚"""

    # åˆ†æ‰¹å¤„ç†æ®µè½
    batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªæ®µè½
    for i in tqdm(range(0, len(unique_sentences), batch_size), desc="åˆ†ææ®µè½"):
        batch = unique_sentences[i:i+batch_size]
        
        # æå–æ¯ä¸ªå¥å­çš„æ®µè½ä¸Šä¸‹æ–‡
        paragraphs = []
        for sentence, p1, p2, line_idx in batch:
            paragraph = extract_paragraph_context(text_lines, line_idx, context_lines)
            paragraphs.append(paragraph)
        
        # åˆå¹¶æ®µè½
        combined_text = "\n\n---æ®µè½åˆ†éš”---\n\n".join(paragraphs)
        
        # è°ƒç”¨ LLM
        try:
            prompt = prompt_template.format(text=combined_text[:3000])  # é™åˆ¶é•¿åº¦
            response = llm.call_api(prompt, max_tokens=2000, temperature=0.3)
            
            # è§£æå“åº”
            try:
                # å°è¯•æå– JSON
                json_match = re.search(r'\[.*\]', response, re.DOTALL)
                if json_match:
                    rel_data = json.loads(json_match.group())
                else:
                    rel_data = json.loads(response)
                
                # å¤„ç†æå–çš„å…³ç³»
                for item in rel_data:
                    if isinstance(item, dict) and 'person1' in item and 'person2' in item:
                        person1 = item.get('person1', '').strip()
                        person2 = item.get('person2', '').strip()
                        relation = item.get('relation', 'ç›¸å…³').strip()
                        
                        if person1 and person2 and person1 != person2:
                            entities.add(person1)
                            entities.add(person2)
                            relationships.append((
                                person1,
                                relation,
                                person2,
                                0.8  # é»˜è®¤ç½®ä¿¡åº¦
                            ))
                
            except json.JSONDecodeError as e:
                print(f"\nâš ï¸ JSON è§£æå¤±è´¥: {e}")
                print(f"   å“åº”å†…å®¹: {response[:200]}")
            
            # é¿å… API é™æµ
            time.sleep(0.5)
            
        except Exception as e:
            print(f"\nâš ï¸ å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nâœ… æå–å®Œæˆ: å‘ç° {len(entities)} ä¸ªäººç‰©ï¼Œ{len(relationships)} ä¸ªå…³ç³»")
    return relationships, list(entities)


# å¤ç”¨åŸæœ‰çš„å…³ç³»å›¾ç»˜åˆ¶å’Œ Excel å¯¼å‡ºå‡½æ•°
def build_relationship_graph(relationships, entities=None):
    """æ„å»ºäººç‰©å…³ç³»å›¾"""
    G = nx.Graph()
    rel_dict = defaultdict(list)
    
    if entities:
        for entity in entities:
            G.add_node(entity)
    
    for person1, relation, person2, confidence in relationships:
        if person1 and person2 and person1 != person2:
            G.add_node(person1)
            G.add_node(person2)
            G.add_edge(person1, person2, weight=1.0, relation=relation, confidence=confidence)
            rel_dict[(person1, person2)].append((relation, confidence))
    
    return G, dict(rel_dict)


def plot_relationship_graph(G, relationships, save_path=None, book_name=None):
    """ç»˜åˆ¶äººç‰©å…³ç³»å›¾"""
    if not G.nodes():
        print("âš ï¸ å›¾ä¸­æ²¡æœ‰èŠ‚ç‚¹ï¼Œæ— æ³•ç»˜åˆ¶")
        return
    
    degrees = dict(G.degree())
    
    if nx.is_connected(G):
        main_G = G
    else:
        components = list(nx.connected_components(G))
        main_component = max(components, key=len)
        main_G = G.subgraph(main_component).copy()
        print(f"ğŸ“Š ä¸»è¦å­å›¾åŒ…å« {len(main_component)} ä¸ªèŠ‚ç‚¹ï¼ˆå…± {len(G.nodes())} ä¸ªèŠ‚ç‚¹ï¼‰")
    
    node_sizes = [degrees.get(node, 1) * 500 for node in main_G.nodes()]
    node_sizes = [max(s, 100) for s in node_sizes]
    
    edge_weights = [G[u][v].get('weight', 1.0) for u, v in main_G.edges()]
    if edge_weights:
        max_weight = max(edge_weights)
        edge_weights = [w * 2.0 / max_weight for w in edge_weights]
    
    num_nodes = len(main_G.nodes())
    if num_nodes > 50:
        figsize = (32, 24)
        font_size = 6
    elif num_nodes > 30:
        figsize = (24, 20)
        font_size = 8
    else:
        figsize = (18, 15)
        font_size = 10
    
    plt.figure(figsize=figsize)
    pos = nx.spring_layout(main_G, k=2, iterations=50)
    
    nx.draw_networkx_nodes(main_G, pos, node_size=node_sizes, 
                          node_color='lightblue', alpha=0.7, 
                          edgecolors='black', linewidths=0.5)
    nx.draw_networkx_edges(main_G, pos, width=edge_weights, 
                          alpha=0.5, edge_color='gray')
    
    labels = {node: node for node in main_G.nodes()}
    nx.draw_networkx_labels(main_G, pos, labels, font_size=font_size,
                           font_family='sans-serif',
                           bbox=dict(boxstyle='round,pad=0.3', 
                                    facecolor='white', 
                                    edgecolor='none', alpha=0.7))
    
    plt.title(f"äººç‰©å…³ç³»å›¾ - {book_name or 'æœªçŸ¥'} (å…±{num_nodes}ä¸ªäººç‰©)", 
              fontsize=14, pad=20)
    plt.axis('off')
    
    if save_path is None:
        save_path = "output"
    os.makedirs(save_path, exist_ok=True)
    
    if book_name:
        safe_book_name = re.sub(r'[<>:"/\\|?*]', '_', book_name)
        filename = os.path.join(save_path, f"{safe_book_name}_relationship.png")
    else:
        filename = os.path.join(save_path, "relationship.png")
    
    plt.savefig(filename, dpi=150, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    print(f"âœ… å·²ä¿å­˜å…³ç³»å›¾: {filename}")
    plt.close()


def export_to_excel(relationships, entities, file_path, book_name=None):
    """å¯¼å‡ºäººç‰©å…³ç³»åˆ° Excel æ–‡ä»¶"""
    rel_data = []
    for person1, relation, person2, confidence in relationships:
        rel_data.append({
            'äººç‰©1': person1,
            'å…³ç³»': relation if relation else 'ç›¸å…³',
            'äººç‰©2': person2,
            'ç½®ä¿¡åº¦': f"{confidence:.4f}" if confidence > 0 else "N/A"
        })
    
    entity_data = []
    entity_counts = defaultdict(int)
    for person1, _, person2, _ in relationships:
        entity_counts[person1] += 1
        entity_counts[person2] += 1
    
    for entity in entities:
        entity_data.append({
            'äººç‰©': entity,
            'å…³ç³»æ•°é‡': entity_counts.get(entity, 0)
        })
    entity_data.sort(key=lambda x: x['å…³ç³»æ•°é‡'], reverse=True)
    
    relation_type_counts = defaultdict(int)
    for _, relation, _, _ in relationships:
        rel_type = relation if relation else 'ç›¸å…³'
        relation_type_counts[rel_type] += 1
    
    rel_type_data = []
    for rel_type, count in sorted(relation_type_counts.items(), 
                                  key=lambda x: x[1], reverse=True):
        rel_type_data.append({
            'å…³ç³»ç±»å‹': rel_type,
            'å‡ºç°æ¬¡æ•°': count
        })
    
    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
        if rel_data:
            df_rel = pd.DataFrame(rel_data)
            df_rel.to_excel(writer, sheet_name='å…³ç³»è¯¦æƒ…', index=False)
        
        if entity_data:
            df_entity = pd.DataFrame(entity_data)
            df_entity.to_excel(writer, sheet_name='äººç‰©ç»Ÿè®¡', index=False)
        
        if rel_type_data:
            df_rel_type = pd.DataFrame(rel_type_data)
            df_rel_type.to_excel(writer, sheet_name='å…³ç³»ç±»å‹ç»Ÿè®¡', index=False)
    
    print(f"âœ… å·²å¯¼å‡º Excel æ–‡ä»¶: {file_path}")


def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶å"""
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    filename = filename.strip(' .')
    if len(filename) > 100:
        filename = filename[:100]
    return filename


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨åœ¨çº¿ API æå–å°è¯´äººç‰©å…³ç³»ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    parser.add_argument("--book", default="å†¬æ—¥é‡ç°", type=str,
                       help="ä¹¦çš„åå­—ï¼Œä¸å¸¦åç¼€")
    parser.add_argument("--base_url", type=str, 
                       default=os.getenv('API_BASE_URL', 'https://miaodi.zeabur.app'),
                       help="API åŸºç¡€ URLï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ API_BASE_URL è¯»å–ï¼‰")
    parser.add_argument("--api_key", type=str, default=None,
                       help="API å¯†é’¥ï¼ˆä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ API_KEY è®¾ç½®ï¼‰")
    parser.add_argument("--model", type=str,
                       default=os.getenv('API_MODEL', 'deepseek-ai/DeepSeek-V3-0324'),
                       help="æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ API_MODEL è¯»å–ï¼‰")
    parser.add_argument("--max_sentences", type=int, default=200,
                       help="æœ€å¤šæå–çš„å¥å­æ•°ï¼ˆé»˜è®¤ 200ï¼‰")
    parser.add_argument("--context_lines", type=int, default=3,
                       help="æ®µè½ä¸Šä¸‹æ–‡è¡Œæ•°ï¼ˆé»˜è®¤ 3ï¼‰")
    parser.add_argument("--output", default="output", type=str,
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # æ–‡ä»¶è·¯å¾„
    fp = f"book/{args.book}.txt"
    if not os.path.exists(fp):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {fp}")
        return
    
    print(f"=====+++=== ä½¿ç”¨ä¼˜åŒ–ç­–ç•¥åˆ†æ: {args.book} ===+++=====")
    print(f"ğŸ“¡ API åœ°å€: {args.base_url}")
    print(f"ğŸ“¦ æ¨¡å‹: {args.model}")
    
    # æå–å…³ç³»
    try:
        relationships, entities = extract_relationships_optimized(
            fp, 
            base_url=args.base_url,
            api_key=args.api_key,
            model_name=args.model,
            max_sentences=args.max_sentences,
            context_lines=args.context_lines
        )
        
        if not relationships:
            print("âš ï¸ æœªæå–åˆ°ä»»ä½•å…³ç³»")
            return
        
        # æ„å»ºå…³ç³»å›¾
        print("\nğŸ“Š æ­£åœ¨æ„å»ºå…³ç³»å›¾...")
        G, rel_dict = build_relationship_graph(relationships, entities)
        
        # ç»˜åˆ¶å…³ç³»å›¾
        print("ğŸ¨ æ­£åœ¨ç»˜åˆ¶å…³ç³»å›¾...")
        os.makedirs(args.output, exist_ok=True)
        plot_relationship_graph(G, relationships, 
                               save_path=args.output, 
                               book_name=args.book)
        
        # å¯¼å‡º Excel
        print("ğŸ“ æ­£åœ¨å¯¼å‡º Excel æ–‡ä»¶...")
        excel_path = os.path.join(args.output, 
                                 f"{sanitize_filename(args.book)}_äººç‰©å…³ç³».xlsx")
        export_to_excel(relationships, entities, excel_path, book_name=args.book)
        
        print("\n" + "="*50)
        print("âœ… å¤„ç†å®Œæˆï¼")
        print(f"   - æå–åˆ° {len(entities)} ä¸ªäººç‰©")
        print(f"   - æå–åˆ° {len(relationships)} ä¸ªå…³ç³»")
        print(f"   - å…³ç³»å›¾å·²ä¿å­˜åˆ°: {args.output}")
        print(f"   - Excel æ–‡ä»¶å·²ä¿å­˜åˆ°: {excel_path}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

